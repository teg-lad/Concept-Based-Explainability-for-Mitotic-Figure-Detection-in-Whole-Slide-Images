{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbfd106",
   "metadata": {},
   "source": [
    "### Automated Concept Extraction\n",
    "\n",
    "There are several steps to the automated concept extraction method that are outlined in their paper [here]().\n",
    "\n",
    "Firstly, we need to create patches from images that represent the object we want to derive concepts for. This involves using skimage segmentation and extracting the patches and superpixels from this. These will be used to find visual features that can be used as concepts.\n",
    "\n",
    "Once we have the patches that we need, we can make use of a clustering technique on the representations extracted from the bottleneck layers of our model after passing a patch through. This will allow us to find visually similar images that will hopefully group patches that represent the same visual concept.\n",
    "\n",
    "These groups of patches can then be used to create a concept activation vector. This involves getting the activations of these ptaches relating to a concept and then random patches that as a group represent no descernable concept. A linear classifier is trained on these examples and the vector orthognal to the hyperplane that separates the concept examples from the random is taken to be the concept activation vector.\n",
    "\n",
    "The influence of this concept can be determined by taking the partial derivative of the class logit you want to examine with respect to a bottleneck layer. Multiplying the CAV by this partial derivative will allow us to determine the impact this concept had on the prediction.\n",
    "\n",
    "This concludes the rough overview of the method that will be employed. The aim is to create realistic, reasonable concepts from the mitotic figures without the need of manually gathering images of specific concepts.\n",
    "\n",
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604bb091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "# from tcav import utils\n",
    "import shutil\n",
    "import torch\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image as im\n",
    "\n",
    "import Utils.ACE.ace_helpers as ace_helpers\n",
    "from Utils.ACE.ace import ConceptDiscovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc5132",
   "metadata": {},
   "source": [
    "### Testing with COCO\n",
    "\n",
    "We will begin by taking some images from the COCO dataset, specifically those inclusing a tennis racket. We will use this example to test our method and ensure we are processing the images correctly and the results seem reasonable. This seems the best course of action as I have a better understanding of the visual features that concern a tennis racket and no formal understanding of the visual features of a mitotic figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88368f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output directory for our data\n",
    "output = Path.cwd() / \"ACE_COCO_output/\"\n",
    "\n",
    "# Create the relevant sub-directories.\n",
    "discovered_concepts_dir = output / 'concepts/'\n",
    "results_dir = output / 'results/'\n",
    "cavs_dir = output / 'cavs/'\n",
    "activations_dir = output / 'acts/'\n",
    "results_summaries_dir = output / 'results_summaries/'\n",
    "\n",
    "# If the directory exists we delete it to generate new output.\n",
    "if output.exists():\n",
    "    shutil.rmtree(output)\n",
    "\n",
    "# Make all of the directories\n",
    "output.mkdir()\n",
    "discovered_concepts_dir.mkdir()\n",
    "results_dir.mkdir()\n",
    "cavs_dir.mkdir()\n",
    "activations_dir.mkdir()\n",
    "results_summaries_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1144ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the target class and the source directory.\n",
    "target_class = \"tennis racket\"\n",
    "source_dir = \"D:\\DS\\DS4\\Project\\COCO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756225f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random concept for statistical testing.\n",
    "random_concept = 'random_discovery'\n",
    "\n",
    "# Create the model variable and set it to evaluate.\n",
    "mymodel = ace_helpers.MyModel()\n",
    "mymodel.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71e58d",
   "metadata": {},
   "source": [
    "### Selecting the bottleneck layers\n",
    "\n",
    "In order to extract the activations and gradients from a layer, we need to determine which layer(s) are bottleneck layers. A bottleneck layer typically reduces the number of channels in the data between the input and output while keeping the size of the image equal by using a kernel of (1,1) and a stride of (1,1). This means that the model compresses the representation of the input in this layer and keeps the most important features for performing the task. This makes it the ideal layer for using the activations from to cluster the patches for ACE and to train the linear classifier for TCAV.\n",
    "\n",
    "Looking at the model structure from above we can see that there are several such layers in the backbone of our model. It may be worth just taking a selection of these. I have decided to take the bottleneck from the last bottleneck unit in each layer. This means I will be using the following 4 layers.\n",
    "\n",
    "```\n",
    "bottleneck_layers = ['body.layer1.2.conv1', 'body.layer2.3.conv1', 'body.layer3.5.conv1', 'body.layer4.2.conv1']\n",
    "```\n",
    "\n",
    "These will be the layers I extract both the activations from and the gradients when looking at the influence of each concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e2fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the names of the different nodes to find the layers we want to extract from.\n",
    "get_graph_node_names(mymodel.model.backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the ConceptDiscovery class instance.\n",
    "cd = ConceptDiscovery(\n",
    "    mymodel,\n",
    "    target_class,\n",
    "    random_concept,\n",
    "    ['body.layer1.2.conv1', 'body.layer2.3.conv1', 'body.layer3.5.conv1', 'body.layer4.2.conv1'],\n",
    "    source_dir,\n",
    "    activations_dir,\n",
    "    cavs_dir,\n",
    "    num_random_exp=2,\n",
    "    channel_mean=True,\n",
    "    max_imgs=100,\n",
    "    min_imgs=50,\n",
    "    num_discovery_imgs=100,\n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b4ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset of image patches.\n",
    "cd.create_patches(discovered_concepts_dir, param_dict={'n_segments': [15]})\n",
    "\n",
    "# Saving the concept discovery target class images.\n",
    "image_dir = discovered_concepts_dir / 'images'\n",
    "image_dir.mkdir()\n",
    "ace_helpers.save_images(image_dir.absolute(),\n",
    "                        (cd.discovery_images * 256).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5c2cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Discovering Concepts\n",
    "cd.discover_concepts(discovered_concepts_dir, method='KM', param_dicts={'n_clusters': 25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38002c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save discovered concept images (resized and original sized)\n",
    "ace_helpers.save_concepts(cd, discovered_concepts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ff21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "superpixels = discovered_concepts_dir / \"superpixels\"\n",
    "list_of_files = list(superpixels.iterdir())\n",
    "\n",
    "# Random selection of the the superpixels for random concept?\n",
    "random.seed(42)\n",
    "cd.random_imgs = np.array(random.sample(list_of_files, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the random imgs for review\n",
    "for img in cd.random_imgs:\n",
    "    destination = img.parent.parent / \"Random\"\n",
    "    destination.mkdir(exist_ok=True)\n",
    "    \n",
    "    shutil.copy(img, destination / img.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe2345",
   "metadata": {},
   "outputs": [],
   "source": [
    "cav_accuracies = cd.cavs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3116786",
   "metadata": {},
   "outputs": [],
   "source": [
    "cav_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating CAVs and TCAV scores\n",
    "cav_accuracies = cd.cavs(min_acc=0.0)\n",
    "scores = cd.tcavs(test=False)\n",
    "ace_helpers.save_ace_report(cd, cav_accuracies, scores,\n",
    "                            results_summaries_dir + 'ace_results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c7a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot examples of discovered concepts\n",
    "for bn in cd.bottlenecks:\n",
    "    ace_helpers.plot_concepts(cd, bn, 10, address=results_dir)\n",
    "# Delete concepts that don't pass statistical testing\n",
    "cd.test_and_remove_concepts(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file Utils/ACE/ace.py\n",
    "\"\"\"ACE library.\n",
    "\n",
    "Library for discovering and testing concept activation vectors. It contains\n",
    "ConceptDiscovery class that is able to discover the concepts belonging to one\n",
    "of the possible classification labels of the classification task of a network\n",
    "and calculate each concept's TCAV score..\n",
    "\"\"\"\n",
    "from multiprocessing import dummy as multiprocessing\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import scipy.stats as stats\n",
    "import skimage.segmentation as segmentation\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.metrics.pairwise as metrics\n",
    "import torch\n",
    "# from tcav import cav\n",
    "from Utils.ACE.ace_helpers import *\n",
    "from Utils.TCAV import cav, tcav\n",
    "\n",
    "\n",
    "class ConceptDiscovery(object):\n",
    "    \"\"\"Discovering and testing concepts of a class.\n",
    "\n",
    "  For a trained network, it first discovers the concepts as areas of the iamges\n",
    "  in the class and then calculates the TCAV score of each concept. It is also\n",
    "  able to transform images from pixel space into concept space.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 target_class,\n",
    "                 random_concept,\n",
    "                 bottlenecks,\n",
    "                 source_dir,\n",
    "                 activation_dir,\n",
    "                 cav_dir,\n",
    "                 num_random_exp=2,\n",
    "                 channel_mean=True,\n",
    "                 max_imgs=40,\n",
    "                 min_imgs=20,\n",
    "                 num_discovery_imgs=40,\n",
    "                 num_workers=20,\n",
    "                 average_image_value=117):\n",
    "        \"\"\"Runs concept discovery for a given class in a trained model.\n",
    "\n",
    "    For a trained classification model, the ConceptDiscovery class first\n",
    "    performs unsupervised concept discovery using examples of one of the classes\n",
    "    in the network.\n",
    "\n",
    "    Args:\n",
    "      model: A trained classification model on which we run the concept\n",
    "             discovery algorithm\n",
    "      target_class: Name of the one of the classes of the network\n",
    "      random_concept: A concept made of random images (used for statistical\n",
    "                      test) e.g. \"random500_199\"\n",
    "      bottlenecks: a list of bottleneck layers of the model for which the cocept\n",
    "                   discovery stage is performed\n",
    "      source_dir: This directory that contains folders with images of network's\n",
    "                  classes.\n",
    "      activation_dir: directory to save computed activations\n",
    "      cav_dir: directory to save CAVs of discovered and random concepts\n",
    "      num_random_exp: Number of random counterparts used for calculating several\n",
    "                      CAVs and TCAVs for each concept (to make statistical\n",
    "                        testing possible.)\n",
    "      channel_mean: If true, for the unsupervised concept discovery the\n",
    "                    bottleneck activations are averaged over channels instead\n",
    "                    of using the whole acivation vector (reducing\n",
    "                    dimensionality)\n",
    "      max_imgs: maximum number of images in a discovered concept\n",
    "      min_imgs : minimum number of images in a discovered concept for the\n",
    "                 concept to be accepted\n",
    "      num_discovery_imgs: Number of images used for concept discovery. If None,\n",
    "                          will use max_imgs instead.\n",
    "      num_workers: if greater than zero, runs methods in parallel with\n",
    "        num_workers parallel threads. If 0, no method is run in parallel\n",
    "        threads.\n",
    "      average_image_value: The average value used for mean subtraction in the\n",
    "                           nework's preprocessing stage.\n",
    "    \"\"\"\n",
    "        self.model = model\n",
    "        self.model.create_feature_extractor(bottlenecks)\n",
    "        self.target_class = target_class\n",
    "        self.num_random_exp = num_random_exp\n",
    "        if isinstance(bottlenecks, str):\n",
    "            bottlenecks = [bottlenecks]\n",
    "        self.bottlenecks = bottlenecks\n",
    "        self.source_dir = Path(source_dir)\n",
    "        self.activation_dir = activation_dir\n",
    "        self.cav_dir = cav_dir\n",
    "        self.channel_mean = channel_mean\n",
    "        self.random_concept = random_concept\n",
    "        self.max_imgs = max_imgs\n",
    "        self.min_imgs = min_imgs\n",
    "        if num_discovery_imgs is None:\n",
    "            num_discovery_imgs = max_imgs\n",
    "        self.num_discovery_imgs = num_discovery_imgs\n",
    "        self.num_workers = num_workers\n",
    "        self.average_image_value = average_image_value\n",
    "\n",
    "    def load_concept_imgs(self, concept, max_imgs=1000):\n",
    "        \"\"\"Loads all colored images of a concept.\n",
    "\n",
    "    Args:\n",
    "      concept: The name of the concept to be loaded\n",
    "      max_imgs: maximum number of images to be loaded\n",
    "\n",
    "    Returns:\n",
    "      Images of the desired concept or class.\n",
    "    \"\"\"\n",
    "        concept_dir = self.source_dir / concept\n",
    "        img_paths = [\n",
    "            os.path.join(concept_dir, d)\n",
    "            for d in concept_dir.iterdir()\n",
    "        ]\n",
    "        return load_images_from_files(\n",
    "            img_paths,\n",
    "            max_imgs=max_imgs,\n",
    "            return_filenames=False,\n",
    "            do_shuffle=False,\n",
    "            run_parallel=(self.num_workers > 0), # image.shape here\n",
    "            num_workers=self.num_workers)\n",
    "\n",
    "    def create_patches(self, concept_dir, method='slic', discovery_images=None,\n",
    "                       param_dict=None):\n",
    "        \"\"\"Creates a set of image patches using superpixel methods.\n",
    "\n",
    "    This method takes in the concept discovery images and transforms it to a\n",
    "    dataset made of the patches of those images.\n",
    "\n",
    "    Args:\n",
    "      method: The superpixel method used for creating image patches. One of\n",
    "        'slic', 'watershed', 'quickshift', 'felzenszwalb'.\n",
    "      discovery_images: Images used for creating patches. If None, the images in\n",
    "        the target class folder are used.\n",
    "\n",
    "      param_dict: Contains parameters of the superpixel method used in the form\n",
    "                of {'param1':[a,b,...], 'param2':[z,y,x,...], ...}. For instance\n",
    "                {'n_segments':[15,50,80], 'compactness':[10,10,10]} for slic\n",
    "                method.\n",
    "    \"\"\"\n",
    "        \n",
    "        superpixel_dir = concept_dir / \"superpixels\"\n",
    "        patch_dir = concept_dir / \"patches\"\n",
    "        \n",
    "        superpixel_dir.mkdir()\n",
    "        patch_dir.mkdir()\n",
    "            \n",
    "        if param_dict is None:\n",
    "            param_dict = {}\n",
    "        dataset, image_numbers, patches = [], [], []\n",
    "        if discovery_images is None:\n",
    "            raw_imgs = self.load_concept_imgs(\n",
    "                self.target_class, self.num_discovery_imgs)\n",
    "            self.discovery_images = raw_imgs\n",
    "        else:\n",
    "            self.discovery_images = discovery_images\n",
    "        if self.num_workers:\n",
    "            pool = multiprocessing.Pool(self.num_workers)\n",
    "            outputs = pool.map(\n",
    "                lambda img: self._return_superpixels(img, method, param_dict),\n",
    "                self.discovery_images)\n",
    "            for fn, sp_outputs in enumerate(outputs):\n",
    "                image_superpixels, image_patches = sp_outputs\n",
    "                for superpixel, patch in zip(image_superpixels, image_patches):\n",
    "                    dataset.append(superpixel)\n",
    "                    patches.append(patch)\n",
    "                    image_numbers.append(fn)\n",
    "        else:\n",
    "            \n",
    "            for fn, img in enumerate(tqdm(self.discovery_images, total=len(self.discovery_images))):\n",
    "                image_superpixels, image_patches = self._return_superpixels(\n",
    "                    img, method, param_dict)\n",
    "                \n",
    "                superpixels, patches = np.array(image_superpixels), np.array(image_patches)\n",
    "                \n",
    "                superpixels = (np.clip(superpixels, 0, 1) * 256).astype(np.uint8)\n",
    "                patches = (np.clip(patches, 0, 1) * 256).astype(np.uint8)\n",
    "            \n",
    "                superpixel_addresses = [superpixel_dir / f\"{fn:03d}_{i:03d}.png\" for i in range(len(image_superpixels))]\n",
    "                patch_addresses = [patch_dir / f\"{fn:03d}_{i:03d}.png\" for i in range(len(image_superpixels))]\n",
    "    \n",
    "                # Save this set\n",
    "                save_images(superpixel_addresses, superpixels)\n",
    "                save_images(patch_addresses, patches)\n",
    "\n",
    "    def _return_superpixels(self, img, method='slic',\n",
    "                            param_dict=None):\n",
    "        \"\"\"Returns all patches for one image.\n",
    "\n",
    "    Given an image, calculates superpixels for each of the parameter lists in\n",
    "    param_dict and returns a set of unique superpixels by\n",
    "    removing duplicates. If two patches have Jaccard similarity more than 0.5,\n",
    "    they are concidered duplicates.\n",
    "\n",
    "    Args:\n",
    "      img: The input image\n",
    "      method: superpixel method, one of slic, watershed, quichsift, or\n",
    "        felzenszwalb\n",
    "      param_dict: Contains parameters of the superpixel method used in the form\n",
    "                of {'param1':[a,b,...], 'param2':[z,y,x,...], ...}. For instance\n",
    "                {'n_segments':[15,50,80], 'compactness':[10,10,10]} for slic\n",
    "                method.\n",
    "    Raises:\n",
    "      ValueError: if the segementation method is invaled.\n",
    "    \"\"\"\n",
    "        if param_dict is None:\n",
    "            param_dict = {}\n",
    "        if method == 'slic':\n",
    "            if \"n_segments\" in param_dict.keys(): \n",
    "                n_segmentss = param_dict[\"n_segments\"]\n",
    "            else:\n",
    "                n_segmentss = [15, 50, 80]\n",
    "            n_params = len(n_segmentss)\n",
    "            compactnesses = param_dict.pop('compactness', [20] * n_params)\n",
    "            sigmas = param_dict.pop('sigma', [1.] * n_params)\n",
    "        elif method == 'watershed':\n",
    "            markerss = param_dict.pop('marker', [15, 50, 80])\n",
    "            n_params = len(markerss)\n",
    "            compactnesses = param_dict.pop('compactness', [0.] * n_params)\n",
    "        elif method == 'quickshift':\n",
    "            max_dists = param_dict.pop('max_dist', [20, 15, 10])\n",
    "            n_params = len(max_dists)\n",
    "            ratios = param_dict.pop('ratio', [1.0] * n_params)\n",
    "            kernel_sizes = param_dict.pop('kernel_size', [10] * n_params)\n",
    "        elif method == 'felzenszwalb':\n",
    "            scales = param_dict.pop('scale', [1200, 500, 250])\n",
    "            n_params = len(scales)\n",
    "            sigmas = param_dict.pop('sigma', [0.8] * n_params)\n",
    "            min_sizes = param_dict.pop('min_size', [20] * n_params)\n",
    "        else:\n",
    "            raise ValueError('Invalid superpixel method!')\n",
    "        unique_masks = []\n",
    "        for i in range(n_params):\n",
    "            param_masks = []\n",
    "            if method == 'slic':\n",
    "                segments = segmentation.slic(\n",
    "                    img, n_segments=n_segmentss[i], compactness=compactnesses[i],\n",
    "                    sigma=sigmas[i])\n",
    "            elif method == 'watershed':\n",
    "                segments = segmentation.watershed(\n",
    "                    img, markers=markerss[i], compactness=compactnesses[i])\n",
    "            elif method == 'quickshift':\n",
    "                segments = segmentation.quickshift(\n",
    "                    img, kernel_size=kernel_sizes[i], max_dist=max_dists[i],\n",
    "                    ratio=ratios[i])\n",
    "            elif method == 'felzenszwalb':\n",
    "                segments = segmentation.felzenszwalb(\n",
    "                    img, scale=scales[i], sigma=sigmas[i], min_size=min_sizes[i])\n",
    "            for s in range(segments.max()):\n",
    "                mask = (segments == s).astype(float)\n",
    "                if np.mean(mask) > 0.001:\n",
    "                    unique = True\n",
    "                    for seen_mask in unique_masks:\n",
    "                        jaccard = np.sum(seen_mask * mask) / np.sum((seen_mask + mask) > 0)\n",
    "                        if jaccard > 0.5:\n",
    "                            unique = False\n",
    "                            break\n",
    "                    if unique:\n",
    "                        param_masks.append(mask)\n",
    "            unique_masks.extend(param_masks)\n",
    "        superpixels, patches = [], []\n",
    "        while unique_masks:\n",
    "            superpixel, patch = self._extract_patch(img, unique_masks.pop())\n",
    "            superpixels.append(superpixel)\n",
    "            patches.append(patch)\n",
    "        return superpixels, patches\n",
    "\n",
    "    def _extract_patch(self, image, mask):\n",
    "        \"\"\"Extracts a patch out of an image.\n",
    "\n",
    "    Args:\n",
    "      image: The original image\n",
    "      mask: The binary mask of the patch area\n",
    "\n",
    "    Returns:\n",
    "      image_resized: The resized patch such that its boundaries touches the\n",
    "        image boundaries\n",
    "      patch: The original patch. Rest of the image is padded with average value\n",
    "    \"\"\"\n",
    "        mask_expanded = np.expand_dims(mask, -1)\n",
    "        patch = (mask_expanded * image + (\n",
    "                1 - mask_expanded) * float(self.average_image_value) / 255)\n",
    "        ones = np.where(mask == 1)\n",
    "        h1, h2, w1, w2 = ones[0].min(), ones[0].max(), ones[1].min(), ones[1].max()\n",
    "        image = Image.fromarray((patch[h1:h2, w1:w2] * 255).astype(np.uint8))\n",
    "        image_resized = np.array(image.resize((299, 299), # image shape here\n",
    "                                              Image.BICUBIC)).astype(float) / 255\n",
    "        return image_resized, patch\n",
    "\n",
    "    def _get_activations(self, img_paths, paths=True, bs=8, channel_mean=None):\n",
    "        \"\"\"Returns activations of a list of imgs.\n",
    "\n",
    "    Args:\n",
    "      imgs: List/array of images to calculate the activations of\n",
    "      bs: The batch size for calculating activations. (To control computational\n",
    "        cost)\n",
    "      channel_mean: If true, the activations are averaged across channel.\n",
    "\n",
    "    Returns:\n",
    "      The array of activations\n",
    "    \"\"\"\n",
    "    \n",
    "        output = []\n",
    "        for i in tqdm(range(ceildiv(img_paths.shape[0], bs)), total=ceildiv(img_paths.shape[0], bs), desc=\"Calculating activations for superpixels\"):\n",
    "            \n",
    "            # Load the images we need\n",
    "            if paths:\n",
    "                imgs = [np.array(Image.open(img)) for img in img_paths[i * bs:(i + 1) * bs]]\n",
    "            else:\n",
    "                imgs = img_paths\n",
    "                \n",
    "            output.append(\n",
    "                self.model.run_examples(np.array(imgs)))\n",
    "\n",
    "        aggregated_out = {}\n",
    "        for k in output[0].keys():\n",
    "            aggregated_out[k] = np.concatenate(list(d[k] for d in output))\n",
    "\n",
    "        return aggregated_out\n",
    "\n",
    "    def _cluster(self, acts, method='KM', param_dict=None):\n",
    "        \"\"\"Runs unsupervised clustering algorithm on concept actiavtations.\n",
    "\n",
    "    Args:\n",
    "      acts: activation vectors of datapoints points in the bottleneck layer.\n",
    "        E.g. (number of clusters,) for Kmeans\n",
    "      method: clustering method. We have:\n",
    "        'KM': Kmeans Clustering\n",
    "        'AP': Affinity Propagation\n",
    "        'SC': Spectral Clustering\n",
    "        'MS': Mean Shift clustering\n",
    "        'DB': DBSCAN clustering method\n",
    "      param_dict: Contains superpixl method's parameters. If an empty dict is\n",
    "                 given, default parameters are used.\n",
    "\n",
    "    Returns:\n",
    "      asg: The cluster assignment label of each data points\n",
    "      cost: The clustering cost of each data point\n",
    "      centers: The cluster centers. For methods like Affinity Propagetion\n",
    "      where they do not return a cluster center or a clustering cost, it\n",
    "      calculates the medoid as the center  and returns distance to center as\n",
    "      each data points clustering cost.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if the clustering method is invalid.\n",
    "    \"\"\"\n",
    "        if param_dict is None:\n",
    "            param_dict = {}\n",
    "        centers = None\n",
    "        if method == 'KM':\n",
    "            n_clusters = param_dict.pop('n_clusters', 25)\n",
    "            km = cluster.KMeans(n_clusters)\n",
    "            d = km.fit(acts)\n",
    "            centers = km.cluster_centers_\n",
    "            d = np.linalg.norm(\n",
    "                np.expand_dims(acts, 1) - np.expand_dims(centers, 0), ord=2, axis=-1)\n",
    "            asg, cost = np.argmin(d, -1), np.min(d, -1)\n",
    "        elif method == 'AP':\n",
    "            damping = param_dict.pop('damping', 0.5)\n",
    "            ca = cluster.AffinityPropagation(damping)\n",
    "            ca.fit(acts)\n",
    "            centers = ca.cluster_centers_\n",
    "            d = np.linalg.norm(\n",
    "                np.expand_dims(acts, 1) - np.expand_dims(centers, 0), ord=2, axis=-1)\n",
    "            asg, cost = np.argmin(d, -1), np.min(d, -1)\n",
    "        elif method == 'MS':\n",
    "            ms = cluster.MeanShift(n_jobs=self.num_workers)\n",
    "            asg = ms.fit_predict(acts)\n",
    "        elif method == 'SC':\n",
    "            n_clusters = param_dict.pop('n_clusters', 25)\n",
    "            sc = cluster.SpectralClustering(\n",
    "                n_clusters=n_clusters, n_jobs=self.num_workers)\n",
    "            asg = sc.fit_predict(acts)\n",
    "        elif method == 'DB':\n",
    "            eps = param_dict.pop('eps', 0.5)\n",
    "            min_samples = param_dict.pop('min_samples', 20)\n",
    "            sc = cluster.DBSCAN(eps, min_samples, n_jobs=self.num_workers)\n",
    "            asg = sc.fit_predict(acts)\n",
    "        else:\n",
    "            raise ValueError('Invalid Clustering Method!')\n",
    "        if centers is None:  ## If clustering returned cluster centers, use medoids\n",
    "            centers = np.zeros((asg.max() + 1, acts.shape[1]))\n",
    "            cost = np.zeros(len(acts))\n",
    "            for cluster_label in range(asg.max() + 1):\n",
    "                cluster_idxs = np.where(asg == cluster_label)[0]\n",
    "                cluster_points = acts[cluster_idxs]\n",
    "                pw_distances = metrics.euclidean_distances(cluster_points)\n",
    "                centers[cluster_label] = cluster_points[np.argmin(\n",
    "                    np.sum(pw_distances, -1))]\n",
    "                cost[cluster_idxs] = np.linalg.norm(\n",
    "                    acts[cluster_idxs] - np.expand_dims(centers[cluster_label], 0),\n",
    "                    ord=2,\n",
    "                    axis=-1)\n",
    "        return asg, cost, centers\n",
    "\n",
    "    def discover_concepts(self, concept_dir,\n",
    "                          method='KM',\n",
    "                          activations=None,\n",
    "                          param_dicts=None):\n",
    "        \"\"\"Discovers the frequent occurring concepts in the target class.\n",
    "\n",
    "      Calculates self.dic, a dicationary containing all the informations of the\n",
    "      discovered concepts in the form of {'bottleneck layer name: bn_dic} where\n",
    "      bn_dic itself is in the form of {'concepts:list of concepts,\n",
    "      'concept name': concept_dic} where the concept_dic is in the form of\n",
    "      {'images': resized patches of concept, 'patches': original patches of the\n",
    "      concepts, 'image_numbers': image id of each patch}\n",
    "\n",
    "    Args:\n",
    "      method: Clustering method.\n",
    "      activations: If activations are already calculated. If not calculates\n",
    "                   them. Must be a dictionary in the form of {'bn':array, ...}\n",
    "      param_dicts: A dictionary in the format of {'bottleneck':param_dict,...}\n",
    "                   where param_dict contains the clustering method's parametrs\n",
    "                   in the form of {'param1':value, ...}. For instance for Kmeans\n",
    "                   {'n_clusters':25}. param_dicts can also be in the format\n",
    "                   of param_dict where same parameters are used for all\n",
    "                   bottlenecks.\n",
    "    \"\"\"\n",
    "        \n",
    "        if param_dicts is None:\n",
    "            param_dicts = {}\n",
    "        if set(param_dicts.keys()) != set(self.bottlenecks):\n",
    "            param_dicts = {bn: param_dicts for bn in self.bottlenecks}\n",
    "            \n",
    "        self.dic = {}  ## The main dictionary of the ConceptDiscovery class.\n",
    "        \n",
    "        if activations is None or set(self.bottlenecks) != set(activations.keys()):\n",
    "            \n",
    "            superpixels_dir = concept_dir / \"superpixels\"\n",
    "            superpixel_images = np.array(list(superpixels_dir.iterdir()))\n",
    "            \n",
    "            patches_dir = concept_dir / \"patches\"\n",
    "            patch_images = np.array(list(patches_dir.iterdir()))\n",
    "            \n",
    "            activations = self._get_activations(superpixel_images)\n",
    "    \n",
    "        # Fill activations\n",
    "        for bn in self.bottlenecks:\n",
    "            bn_dic = {}\n",
    "            bn_activations = activations[bn]\n",
    "\n",
    "            bn_dic['label'], bn_dic['cost'], centers = self._cluster(\n",
    "                bn_activations, method, param_dicts[bn])\n",
    "            concept_number, bn_dic['concepts'] = 0, []\n",
    "            for i in range(bn_dic['label'].max() + 1):\n",
    "                label_idxs = np.where(bn_dic['label'] == i)[0]\n",
    "                if len(label_idxs) > self.min_imgs:\n",
    "                    concept_costs = bn_dic['cost'][label_idxs]\n",
    "                    concept_idxs = label_idxs[np.argsort(concept_costs)[:self.max_imgs]]\n",
    "                    concept_image_numbers = set([int(p.name.split(\"_\")[0]) for p in patch_images[label_idxs]])\n",
    "                    discovery_size = len(self.discovery_images)\n",
    "                    highly_common_concept = len(\n",
    "                        concept_image_numbers) > 0.5 * len(label_idxs)\n",
    "                    mildly_common_concept = len(\n",
    "                        concept_image_numbers) > 0.25 * len(label_idxs)\n",
    "                    mildly_populated_concept = len(\n",
    "                        concept_image_numbers) > 0.25 * discovery_size\n",
    "                    cond2 = mildly_populated_concept and mildly_common_concept\n",
    "                    non_common_concept = len(\n",
    "                        concept_image_numbers) > 0.1 * len(label_idxs)\n",
    "                    highly_populated_concept = len(\n",
    "                        concept_image_numbers) > 0.5 * discovery_size\n",
    "                    cond3 = non_common_concept and highly_populated_concept\n",
    "                    if highly_common_concept or cond2 or cond3:\n",
    "                        concept_number += 1\n",
    "                        concept = '{}_concept{}'.format(self.target_class, concept_number)\n",
    "                        bn_dic['concepts'].append(concept)\n",
    "                        bn_dic[concept] = {\n",
    "                            'images': superpixel_images[concept_idxs],\n",
    "                            'patches': patch_images[concept_idxs],\n",
    "                            'image_numbers': [str(p.name.split(\".\")[0]) for p in patch_images[concept_idxs]]\n",
    "                        }\n",
    "                        bn_dic[concept + '_center'] = centers[i]\n",
    "            bn_dic.pop('label', None)\n",
    "            bn_dic.pop('cost', None)\n",
    "            self.dic[bn] = bn_dic\n",
    "\n",
    "    def _random_concept_activations(self, bottleneck, random_concept):\n",
    "        \"\"\"Wrapper for computing or loading activations of random concepts.\n",
    "\n",
    "    Takes care of making, caching (if desired) and loading activations.\n",
    "\n",
    "    Args:\n",
    "      bottleneck: The bottleneck layer name\n",
    "      random_concept: Name of the random concept e.g. \"random500_0\"\n",
    "\n",
    "    Returns:\n",
    "      A nested dict in the form of {concept:{bottleneck:activation}}\n",
    "    \"\"\"\n",
    "        rnd_acts_path = os.path.join(self.activation_dir, 'acts_{}_{}'.format(\n",
    "            random_concept, bottleneck))\n",
    "        if not tf.gfile.Exists(rnd_acts_path):\n",
    "            rnd_imgs = self.load_concept_imgs(random_concept, self.max_imgs)\n",
    "            acts = get_acts_from_images(rnd_imgs, self.model, bottleneck)\n",
    "            with tf.gfile.Open(rnd_acts_path, 'w') as f:\n",
    "                np.save(f, acts, allow_pickle=False)\n",
    "            del acts\n",
    "            del rnd_imgs\n",
    "        return np.load(rnd_acts_path).squeeze()\n",
    "\n",
    "    def _calculate_cav(self, c, r, bn, act_c, act_r, ow, directory=None):\n",
    "        \"\"\"Calculates a sinle cav for a concept and a one random counterpart.\n",
    "\n",
    "    Args:\n",
    "      c: conept name\n",
    "      r: random concept name\n",
    "      bn: the layer name\n",
    "      act_c: activation matrix of the concept in the 'bn' layer\n",
    "      ow: overwrite if CAV already exists\n",
    "      directory: to save the generated CAV\n",
    "\n",
    "    Returns:\n",
    "      The accuracy of the CAV\n",
    "    \"\"\"\n",
    "        if directory is None:\n",
    "            directory = self.cav_dir\n",
    "#         act_r = self._random_concept_activations(bn, r)\n",
    "\n",
    "        cav_instance = cav.load_or_train_cav([c, r], bn, directory, \n",
    "                                             activations={c: {bn: act_c}, r: {bn: act_r}},\n",
    "                                             overwrite=ow)\n",
    "\n",
    "        return cav_instance.accuracies['overall']\n",
    "\n",
    "    def _concept_cavs(self, bn, concept, activations, random_activations, randoms=None, ow=True):\n",
    "        \"\"\"Calculates CAVs of a concept versus all the random counterparts.\n",
    "\n",
    "    Args:\n",
    "      bn: bottleneck layer name\n",
    "      concept: the concept name\n",
    "      activations: activations of the concept in the bottleneck layer\n",
    "      randoms: None if the class random concepts are going to be used\n",
    "      ow: If true, overwrites the existing CAVs\n",
    "\n",
    "    Returns:\n",
    "      A dict of cav accuracies in the form of {'bottleneck layer':\n",
    "      {'concept name':[list of accuracies], ...}, ...}\n",
    "    \"\"\"\n",
    "        if randoms is None:\n",
    "            randoms = [\n",
    "                'random500_{}'.format(i) for i in np.arange(self.num_random_exp)\n",
    "            ]\n",
    "        rnd = \"Random\"\n",
    "        \n",
    "        if self.num_workers:\n",
    "            pool = multiprocessing.Pool(20)\n",
    "            accs = pool.map(\n",
    "                lambda rnd: self._calculate_cav(concept, rnd, bn, activations, random_activations, ow),\n",
    "                randoms)\n",
    "        else:\n",
    "#             accs = []\n",
    "#             for rnd in randoms:\n",
    "            accs = self._calculate_cav(concept, rnd, bn, activations, random_activations,  ow)\n",
    "        return accs\n",
    "\n",
    "    def cavs(self, min_acc=0, ow=True):\n",
    "        \"\"\"Calculates cavs for all discovered concepts.\n",
    "\n",
    "    This method calculates and saves CAVs for all the discovered concepts\n",
    "    versus all random concepts in all the bottleneck layers\n",
    "\n",
    "    Args:\n",
    "      min_acc: Delete discovered concept if the average classification accuracy\n",
    "        of the CAV is less than min_acc\n",
    "      ow: If True, overwrites an already calcualted cav.\n",
    "\n",
    "    Returns:\n",
    "      A dicationary of classification accuracy of linear boundaries orthogonal\n",
    "      to cav vectors\n",
    "    \"\"\"\n",
    "        acc = {bn: {} for bn in self.bottlenecks}\n",
    "        concepts_to_delete = []\n",
    "        \n",
    "        target_class_acts_all = self._get_activations(self.discovery_images, paths=False)\n",
    "        \n",
    "        # Get all images in the random concept folder\n",
    "        # List of all random images in the random concept folder.\n",
    "        # random_concept_imgs = iter over directory\n",
    "        rnd_acts_all = self._get_activations(self.random_imgs)\n",
    "        \n",
    "        for bn in self.bottlenecks:\n",
    "            \n",
    "#             target_class_acts = target_class_acts_all[bn]\n",
    "#             acc[bn][self.target_class] = self._concept_cavs(\n",
    "#                 bn, self.target_class, target_class_acts, ow=ow)\n",
    "            \n",
    "            rnd_acts = rnd_acts_all[bn]\n",
    "#             acc[bn][self.random_concept] = self._concept_cavs(\n",
    "#                 bn, self.random_concept, rnd_acts, ow=ow)\n",
    "            \n",
    "            for concept in self.dic[bn]['concepts']:\n",
    "                \n",
    "                concept_imgs = self.dic[bn][concept]['images']\n",
    "                concept_acts_all = self._get_activations(concept_imgs)\n",
    "                \n",
    "                concept_acts = concept_acts_all[bn]\n",
    "                acc[bn][concept] = self._concept_cavs(bn, concept, concept_acts, rnd_acts, ow=ow)\n",
    "                if np.mean(acc[bn][concept]) < min_acc:\n",
    "                    concepts_to_delete.append((bn, concept))\n",
    "            \n",
    "        for bn, concept in concepts_to_delete:\n",
    "            self.delete_concept(bn, concept)\n",
    "            \n",
    "        return acc\n",
    "\n",
    "    def load_cav_direction(self, c, r, bn, directory=None):\n",
    "        \"\"\"Loads an already computed cav.\n",
    "\n",
    "    Args:\n",
    "      c: concept name\n",
    "      r: random concept name\n",
    "      bn: bottleneck layer\n",
    "      directory: where CAV is saved\n",
    "\n",
    "    Returns:\n",
    "      The cav instance\n",
    "    \"\"\"\n",
    "        if directory is None:\n",
    "            directory = self.cav_dir\n",
    "            \n",
    "        params = tf.contrib.training.HParams(model_type='linear', alpha=.01)\n",
    "        cav_key = cav.CAV.cav_key([c, r], bn, params.model_type, params.alpha)\n",
    "        cav_path = os.path.join(self.cav_dir, cav_key.replace('/', '.') + '.pkl')\n",
    "        vector = cav.CAV.load_cav(cav_path).cavs[0]\n",
    "        return np.expand_dims(vector, 0) / np.linalg.norm(vector, ord=2)\n",
    "\n",
    "    def _sort_concepts(self, scores):\n",
    "        for bn in self.bottlenecks:\n",
    "            tcavs = []\n",
    "            for concept in self.dic[bn]['concepts']:\n",
    "                tcavs.append(np.mean(scores[bn][concept]))\n",
    "            concepts = []\n",
    "            for idx in np.argsort(tcavs)[::-1]:\n",
    "                concepts.append(self.dic[bn]['concepts'][idx])\n",
    "            self.dic[bn]['concepts'] = concepts\n",
    "\n",
    "    def _return_gradients(self, images):\n",
    "        \"\"\"For the given images calculates the gradient tensors.\n",
    "\n",
    "    Args:\n",
    "      images: Images for which we want to calculate gradients.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary of images gradients in all bottleneck layers.\n",
    "    \"\"\"\n",
    "        \n",
    "        output = []\n",
    "        \n",
    "        class_id = self.model.label_to_id(self.target_class.replace('_', ' '))\n",
    "        \n",
    "        for i in tqdm(range(len(images)), total=len(images, desc=\"Calculating gradients\"):\n",
    "            \n",
    "            # Load the image we need\n",
    "            if paths:\n",
    "                imgs = [np.array(Image.open(img))]\n",
    "            else:\n",
    "                imgs = img_paths\n",
    "                \n",
    "            output.append(\n",
    "                self.model.get_gradient(np.array(imgs), class_id, self.bottlenecks))\n",
    "\n",
    "        aggregated_out = {}\n",
    "        for k in output[0].keys():\n",
    "            aggregated_out[k] = np.concatenate(list(d[k] for d in output))\n",
    "\n",
    "        return aggregated_out  \n",
    "\n",
    "    def _tcav_score(self, bn, concept, rnd, gradients):\n",
    "        \"\"\"Calculates and returns the TCAV score of a concept.\n",
    "\n",
    "    Args:\n",
    "      bn: bottleneck layer\n",
    "      concept: concept name\n",
    "      rnd: random counterpart\n",
    "      gradients: Dict of gradients of tcav_score_images\n",
    "\n",
    "    Returns:\n",
    "      TCAV score of the concept with respect to the given random counterpart\n",
    "    \"\"\"\n",
    "        vector = self.load_cav_direction(concept, rnd, bn)\n",
    "        prod = np.sum(gradients[bn] * vector, -1)\n",
    "        return np.mean(prod < 0)\n",
    "\n",
    "    def tcavs(self, test=False, sort=True, tcav_score_images=None):\n",
    "        \"\"\"Calculates TCAV scores for all discovered concepts and sorts concepts.\n",
    "\n",
    "    This method calculates TCAV scores of all the discovered concepts for\n",
    "    the target class using all the calculated cavs. It later sorts concepts\n",
    "    based on their TCAV scores.\n",
    "\n",
    "    Args:\n",
    "      test: If true, perform statistical testing and removes concepts that don't\n",
    "        pass\n",
    "      sort: If true, it will sort concepts in each bottleneck layers based on\n",
    "        average TCAV score of the concept.\n",
    "      tcav_score_images: Target class images used for calculating tcav scores.\n",
    "        If None, the target class source directory images are used.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary of the form {'bottleneck layer':{'concept name':\n",
    "      [list of tcav scores], ...}, ...} containing TCAV scores.\n",
    "    \"\"\"\n",
    "\n",
    "        tcav_scores = {bn: {} for bn in self.bottlenecks}\n",
    "        \n",
    "        randoms = ['random500_{}'.format(i) for i in np.arange(self.num_random_exp)]\n",
    "        \n",
    "        if tcav_score_images is None:  # Load target class images if not given\n",
    "            raw_imgs = self.load_concept_imgs(self.target_class, 2 * self.max_imgs)\n",
    "            tcav_score_images = raw_imgs[-self.max_imgs:]\n",
    "        \n",
    "        # Accept image paths from target class folder?\n",
    "        gradients = self._return_gradients(tcav_score_images)\n",
    "        \n",
    "        for bn in self.bottlenecks:\n",
    "            for concept in self.dic[bn]['concepts'] + [self.random_concept]:\n",
    "                def t_func(rnd):\n",
    "                    return self._tcav_score(bn, concept, rnd, gradients)\n",
    "\n",
    "                if self.num_workers:\n",
    "                    pool = multiprocessing.Pool(self.num_workers)\n",
    "                    tcav_scores[bn][concept] = pool.map(lambda rnd: t_func(rnd), randoms)\n",
    "                else:\n",
    "                    tcav_scores[bn][concept] = [t_func(rnd) for rnd in randoms]\n",
    "        if test:\n",
    "            self.test_and_remove_concepts(tcav_scores)\n",
    "        if sort:\n",
    "            self._sort_concepts(tcav_scores)\n",
    "        return tcav_scores\n",
    "\n",
    "    def do_statistical_testings(self, i_ups_concept, i_ups_random):\n",
    "        \"\"\"Conducts ttest to compare two set of samples.\n",
    "\n",
    "    In particular, if the means of the two samples are staistically different.\n",
    "\n",
    "    Args:\n",
    "      i_ups_concept: samples of TCAV scores for concept vs. randoms\n",
    "      i_ups_random: samples of TCAV scores for random vs. randoms\n",
    "\n",
    "    Returns:\n",
    "      p value\n",
    "    \"\"\"\n",
    "        min_len = min(len(i_ups_concept), len(i_ups_random))\n",
    "        _, p = stats.ttest_rel(i_ups_concept[:min_len], i_ups_random[:min_len])\n",
    "        return p\n",
    "\n",
    "    def test_and_remove_concepts(self, tcav_scores):\n",
    "        \"\"\"Performs statistical testing for all discovered concepts.\n",
    "\n",
    "    Using TCAV socres of the discovered concepts versurs the random_counterpart\n",
    "    concept, performs statistical testing and removes concepts that do not pass\n",
    "\n",
    "    Args:\n",
    "      tcav_scores: Calculated dicationary of tcav scores of all concepts\n",
    "    \"\"\"\n",
    "        concepts_to_delete = []\n",
    "        for bn in self.bottlenecks:\n",
    "            for concept in self.dic[bn]['concepts']:\n",
    "                pvalue = self.do_statistical_testings \\\n",
    "                    (tcav_scores[bn][concept], tcav_scores[bn][self.random_concept])\n",
    "                if pvalue > 0.01:\n",
    "                    concepts_to_delete.append((bn, concept))\n",
    "        for bn, concept in concepts_to_delete:\n",
    "            self.delete_concept(bn, concept)\n",
    "\n",
    "    def delete_concept(self, bn, concept):\n",
    "        \"\"\"Removes a discovered concepts if it's not already removed.\n",
    "\n",
    "    Args:\n",
    "      bn: Bottleneck layer where the concepts is discovered.\n",
    "      concept: concept name\n",
    "    \"\"\"\n",
    "        self.dic[bn].pop(concept, None)\n",
    "        if concept in self.dic[bn]['concepts']:\n",
    "            self.dic[bn]['concepts'].pop(self.dic[bn]['concepts'].index(concept))\n",
    "\n",
    "    def _concept_profile(self, bn, activations, concept, randoms):\n",
    "        \"\"\"Transforms data points from activations space to concept space.\n",
    "\n",
    "    Calculates concept profile of data points in the desired bottleneck\n",
    "    layer's activation space for one of the concepts\n",
    "\n",
    "    Args:\n",
    "      bn: Bottleneck layer\n",
    "      activations: activations of the data points in the bottleneck layer\n",
    "      concept: concept name\n",
    "      randoms: random concepts\n",
    "\n",
    "    Returns:\n",
    "      The projection of activations of all images on all CAV directions of\n",
    "        the given concept\n",
    "    \"\"\"\n",
    "\n",
    "        def t_func(rnd):\n",
    "            products = self.load_cav_direction(concept, rnd, bn) * activations\n",
    "            return np.sum(products, -1)\n",
    "\n",
    "        if self.num_workers:\n",
    "            pool = multiprocessing.Pool(self.num_workers)\n",
    "            profiles = pool.map(lambda rnd: t_func(rnd), randoms)\n",
    "        else:\n",
    "            profiles = [t_func(rnd) for rnd in randoms]\n",
    "        return np.stack(profiles, axis=-1)\n",
    "\n",
    "    def find_profile(self, bn, images, mean=True):\n",
    "        \"\"\"Transforms images from pixel space to concept space.\n",
    "\n",
    "    Args:\n",
    "      bn: Bottleneck layer\n",
    "      images: Data points to be transformed\n",
    "      mean: If true, the profile of each concept would be the average inner\n",
    "        product of all that concepts' CAV vectors rather than the stacked up\n",
    "        version.\n",
    "\n",
    "    Returns:\n",
    "      The concept profile of input images in the bn layer.\n",
    "    \"\"\"\n",
    "        profile = np.zeros((len(images), len(self.dic[bn]['concepts']),\n",
    "                            self.num_random_exp))\n",
    "        class_acts = get_acts_from_images(\n",
    "            images, self.model, bn).reshape([len(images), -1])\n",
    "        randoms = ['random500_{}'.format(i) for i in range(self.num_random_exp)]\n",
    "        for i, concept in enumerate(self.dic[bn]['concepts']):\n",
    "            profile[:, i, :] = self._concept_profile(bn, class_acts, concept, randoms)\n",
    "        if mean:\n",
    "            profile = np.mean(profile, -1)\n",
    "        return profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb09014",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file Utils/ACE/ace_helpers.py\n",
    "\"\"\" collection of various helper functions for running ACE\"\"\"\n",
    "\n",
    "from multiprocessing import dummy as multiprocessing\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "# from matplotlib import pyplot as plt\n",
    "# import matplotlib.gridspec as gridspec\n",
    "# import tcav.model as model\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "# from skimage.segmentation import mark_boundaries\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from pathlib import Path\n",
    "\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class MyModel():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        \n",
    "        for param in self.model.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.feature_extractor = None\n",
    "\n",
    "    def create_feature_extractor(self, bottlenecks):\n",
    "        self.feature_extractor = create_feature_extractor(self.model.backbone, bottlenecks)\n",
    "        self.feature_extractor.to(self.device)\n",
    "\n",
    "    def run_examples(self, imgs, bn=None):\n",
    "\n",
    "        tensor_imgs = torch.from_numpy(imgs).float()\n",
    "\n",
    "        if len(tensor_imgs.shape) < 4:    \n",
    "            tensor_imgs = tensor_imgs[None, :]\n",
    "        \n",
    "        \n",
    "        tensor_imgs = tensor_imgs.permute(0, 3, 1, 2)\n",
    "        \n",
    "        tensor_imgs = tensor_imgs.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = self.feature_extractor(tensor_imgs)\n",
    "        \n",
    "        flattened = {k: torch.flatten(torch.mean(v.detach(), dim=1).cpu(), start_dim=1) for k, v in out.items()}\n",
    "        output = {k: list(v.numpy()) for k, v in flattened.items()}\n",
    "        \n",
    "        if bn:\n",
    "            return output[bn]\n",
    "        else:  \n",
    "            return {k: list(v.numpy()) for k, v in flattened.items()}\n",
    "    \n",
    "    def label_to_id(self, label):\n",
    "        default = {\"tennis racket\": 43}\n",
    "        return default[label]\n",
    "    \n",
    "    def get_gradient(self, imgs, class_id, bns)\n",
    "            \n",
    "        tensor_imgs = torch.from_numpy(imgs).float()\n",
    "\n",
    "        if len(tensor_imgs.shape) < 4:    \n",
    "            tensor_imgs = tensor_imgs[None, :]\n",
    "        \n",
    "        tensor_imgs = tensor_imgs.permute(0, 3, 1, 2)\n",
    "        \n",
    "        tensor_imgs = tensor_imgs.to(self.device)\n",
    "\n",
    "        self.model.to(device)\n",
    "        predictions, class_logits = self.model(tensor_imgs)\n",
    "        \n",
    "        gradients = []\n",
    "        \n",
    "        for entry in range(class_logits.shape[1]):\n",
    "            self.model.zero_grad()\n",
    "            \n",
    "            class_logits[entry, class_id].backward(retain_graph=True)\n",
    "            \n",
    "            grads = return_grads(bns)\n",
    "            \n",
    "        flattened = {k: torch.flatten(torch.mean(v.detach(), dim=1).cpu(), start_dim=1) for k, v in out.items()}\n",
    "        output = {k: list(v.numpy()) for k, v in flattened.items()}\n",
    "        \n",
    "        if bn:\n",
    "            return output[bn]\n",
    "        else:  \n",
    "            return {k: list(v.numpy()) for k, v in flattened.items()}\n",
    "    \n",
    "    def return_grads(self, bns):\n",
    "        grads = {}\n",
    "        \n",
    "        for bn in bns:\n",
    "            \n",
    "            body_attribute, layer_attribute, layer_number_attribute, component_atribute = bn.split(\".\")\n",
    "            body = getattr(self.model.backbone, body_attribute)\n",
    "            layer = getattr(body_attr, layer_attribute)\n",
    "            layer_number = layer_attr[int(layer_number_attribute)]\n",
    "            component = getattr(layer_number, component_atribute)\n",
    "            grads[bn] = component.weight.grad.cpu().detach().numpy()\n",
    "\n",
    "        return grads\n",
    "            \n",
    "\n",
    "def load_image_from_file(filename, shape):\n",
    "    \"\"\"Given a filename, try to open the file. If failed, return None.\n",
    "  Args:\n",
    "    filename: location of the image file\n",
    "    shape: the shape of the image file to be scaled\n",
    "  Returns:\n",
    "    the image if succeeds, None if fails.\n",
    "  Rasies:\n",
    "    exception if the image was not the right shape.\n",
    "  \"\"\"\n",
    "    if not Path(filename).exists():\n",
    "        print('Cannot find file: {}'.format(filename))\n",
    "        return None\n",
    "    try:\n",
    "        img = np.array(Image.open(filename).resize(\n",
    "            shape, Image.BILINEAR))\n",
    "        # Normalize pixel values to between 0 and 1.\n",
    "        img = np.float32(img) / 255.0\n",
    "        if not (len(img.shape) == 3 and img.shape[2] == 3):\n",
    "            return None\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    except Exception as e:\n",
    "        tf.logging.info(e)\n",
    "        return None\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_images_from_files(filenames, max_imgs=500, return_filenames=False,\n",
    "                           do_shuffle=True, run_parallel=True,\n",
    "                           shape=(299, 299),\n",
    "                           num_workers=100):\n",
    "    \"\"\"Return image arrays from filenames.\n",
    "  Args:\n",
    "    filenames: locations of image files.\n",
    "    max_imgs: maximum number of images from filenames.\n",
    "    return_filenames: return the succeeded filenames or not\n",
    "    do_shuffle: before getting max_imgs files, shuffle the names or not\n",
    "    run_parallel: get images in parallel or not\n",
    "    shape: desired shape of the image\n",
    "    num_workers: number of workers in parallelization.\n",
    "  Returns:\n",
    "    image arrays and succeeded filenames if return_filenames=True.\n",
    "  \"\"\"\n",
    "    imgs = []\n",
    "    # First shuffle a copy of the filenames.\n",
    "    filenames = filenames[:]\n",
    "    if do_shuffle:\n",
    "        np.random.shuffle(filenames)\n",
    "    if return_filenames:\n",
    "        final_filenames = []\n",
    "    if run_parallel:\n",
    "        pool = multiprocessing.Pool(num_workers)\n",
    "        imgs = pool.map(lambda filename: load_image_from_file(filename, shape),\n",
    "                        filenames[:max_imgs])\n",
    "        if return_filenames:\n",
    "            final_filenames = [f for i, f in enumerate(filenames[:max_imgs])\n",
    "                               if imgs[i] is not None]\n",
    "        imgs = [img for img in imgs if img is not None]\n",
    "    else:\n",
    "        for filename in filenames:\n",
    "            img = load_image_from_file(filename, shape)\n",
    "            if img is not None:\n",
    "                imgs.append(img)\n",
    "                if return_filenames:\n",
    "                    final_filenames.append(filename)\n",
    "            if len(imgs) >= max_imgs:\n",
    "                break\n",
    "\n",
    "    if return_filenames:\n",
    "        return np.array(imgs), final_filenames\n",
    "    else:\n",
    "        return np.array(imgs)\n",
    "\n",
    "\n",
    "def get_acts_from_images(imgs, model, bottleneck_name):\n",
    "    \"\"\"Run images in the model to get the activations.\n",
    "  Args:\n",
    "    imgs: a list of images\n",
    "    model: a model instance\n",
    "    bottleneck_name: bottleneck name to get the activation from\n",
    "  Returns:\n",
    "    numpy array of activations.\n",
    "  \"\"\"\n",
    "    return model.run_examples(imgs, bottleneck_name)\n",
    "\n",
    "\n",
    "def flat_profile(cd, images, bottlenecks=None):\n",
    "    \"\"\"Returns concept profile of given images.\n",
    "\n",
    "  Given a ConceptDiscovery class instance and a set of images, and desired\n",
    "  bottleneck layers, calculates the profile of each image with all concepts and\n",
    "  returns a profile vector\n",
    "\n",
    "  Args:\n",
    "    cd: The concept discovery class instance\n",
    "    images: The images for which the concept profile is calculated\n",
    "    bottlenecks: Bottleck layers where the profile is calculated. If None, cd\n",
    "      bottlenecks will be used.\n",
    "\n",
    "  Returns:\n",
    "    The concepts profile of input images using discovered concepts in\n",
    "    all bottleneck layers.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If bottlenecks is not in right format.\n",
    "  \"\"\"\n",
    "    profiles = []\n",
    "    if bottlenecks is None:\n",
    "        bottlenecks = list(cd.dic.keys())\n",
    "    if isinstance(bottlenecks, str):\n",
    "        bottlenecks = [bottlenecks]\n",
    "    elif not isinstance(bottlenecks, list) and not isinstance(bottlenecks, tuple):\n",
    "        raise ValueError('Invalid bottlenecks parameter!')\n",
    "    for bn in bottlenecks:\n",
    "        profiles.append(cd.find_profile(str(bn), images).reshape((len(images), -1)))\n",
    "    profile = np.concatenate(profiles, -1)\n",
    "    return profile\n",
    "\n",
    "\n",
    "def cross_val(a, b, methods):\n",
    "    \"\"\"Performs cross validation for a binary classification task.\n",
    "\n",
    "  Args:\n",
    "    a: First class data points as rows\n",
    "    b: Second class data points as rows\n",
    "    methods: The sklearn classification models to perform cross-validation on\n",
    "\n",
    "  Returns:\n",
    "    The best performing trained binary classification odel\n",
    "  \"\"\"\n",
    "    x, y = binary_dataset(a, b)\n",
    "    best_acc = 0.\n",
    "    if isinstance(methods, str):\n",
    "        methods = [methods]\n",
    "    best_acc = 0.\n",
    "    for method in methods:\n",
    "        temp_acc = 0.\n",
    "        params = [10 ** e for e in [-4, -3, -2, -1, 0, 1, 2, 3]]\n",
    "        for param in params:\n",
    "            clf = give_classifier(method, param)\n",
    "            acc = cross_val_score(clf, x, y, cv=min(100, max(2, int(len(y) / 10))))\n",
    "            if np.mean(acc) > temp_acc:\n",
    "                temp_acc = np.mean(acc)\n",
    "                best_param = param\n",
    "        if temp_acc > best_acc:\n",
    "            best_acc = temp_acc\n",
    "            final_clf = give_classifier(method, best_param)\n",
    "    final_clf.fit(x, y)\n",
    "    return final_clf, best_acc\n",
    "\n",
    "\n",
    "def give_classifier(method, param):\n",
    "    \"\"\"Returns an sklearn classification model.\n",
    "\n",
    "  Args:\n",
    "    method: Name of the sklearn classification model\n",
    "    param: Hyperparameters of the sklearn model\n",
    "\n",
    "  Returns:\n",
    "    An untrained sklearn classification model\n",
    "\n",
    "  Raises:\n",
    "    ValueError: if the model name is invalid.\n",
    "  \"\"\"\n",
    "    if method == 'logistic':\n",
    "        return linear_model.LogisticRegression(C=param)\n",
    "    elif method == 'sgd':\n",
    "        return linear_model.SGDClassifier(alpha=param)\n",
    "    else:\n",
    "        raise ValueError('Invalid model!')\n",
    "\n",
    "\n",
    "def binary_dataset(pos, neg, balanced=True):\n",
    "    \"\"\"Creates a binary dataset given instances of two classes.\n",
    "\n",
    "  Args:\n",
    "     pos: Data points of the first class as rows\n",
    "     neg: Data points of the second class as rows\n",
    "     balanced: If true, it creates a balanced binary dataset.\n",
    "\n",
    "  Returns:\n",
    "    The data points of the created data set as rows and the corresponding labels\n",
    "  \"\"\"\n",
    "    if balanced:\n",
    "        min_len = min(neg.shape[0], pos.shape[0])\n",
    "        ridxs = np.random.permutation(np.arange(2 * min_len))\n",
    "        x = np.concatenate([neg[:min_len], pos[:min_len]], 0)[ridxs]\n",
    "        y = np.concatenate([np.zeros(min_len), np.ones(min_len)], 0)[ridxs]\n",
    "    else:\n",
    "        ridxs = np.random.permutation(np.arange(len(neg) + len(pos)))\n",
    "        x = np.concatenate([neg, pos], 0)[ridxs]\n",
    "        y = np.concatenate(\n",
    "            [np.zeros(neg.shape[0]), np.ones(pos.shape[0])], 0)[ridxs]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_concepts(cd, bn, num=10, address=None, mode='diverse', concepts=None):\n",
    "    \"\"\"Plots examples of discovered concepts.\n",
    "\n",
    "  Args:\n",
    "    cd: The concept discovery instance\n",
    "    bn: Bottleneck layer name\n",
    "    num: Number of images to print out of each concept\n",
    "    address: If not None, saves the output to the address as a .PNG image\n",
    "    mode: If 'diverse', it prints one example of each of the target class images\n",
    "      is coming from. If 'radnom', randomly samples exmples of the concept. If\n",
    "      'max', prints out the most activating examples of that concept.\n",
    "    concepts: If None, prints out examples of all discovered concepts.\n",
    "      Otherwise, it should be either a list of concepts to print out examples of\n",
    "      or just one concept's name\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If the mode is invalid.\n",
    "  \"\"\"\n",
    "    if concepts is None:\n",
    "        concepts = cd.dic[bn]['concepts']\n",
    "    elif not isinstance(concepts, list) and not isinstance(concepts, tuple):\n",
    "        concepts = [concepts]\n",
    "    num_concepts = len(concepts)\n",
    "    plt.rcParams['figure.figsize'] = num * 2.1, 4.3 * num_concepts\n",
    "    fig = plt.figure(figsize=(num * 2, 4 * num_concepts))\n",
    "    outer = gridspec.GridSpec(num_concepts, 1, wspace=0., hspace=0.3)\n",
    "    for n, concept in enumerate(concepts):\n",
    "        inner = gridspec.GridSpecFromSubplotSpec(\n",
    "            2, num, subplot_spec=outer[n], wspace=0, hspace=0.1)\n",
    "        concept_images = cd.dic[bn][concept]['images']\n",
    "        concept_patches = cd.dic[bn][concept]['patches']\n",
    "        concept_image_numbers = cd.dic[bn][concept]['image_numbers']\n",
    "        if mode == 'max':\n",
    "            idxs = np.arange(len(concept_images))\n",
    "        elif mode == 'random':\n",
    "            idxs = np.random.permutation(np.arange(len(concept_images)))\n",
    "        elif mode == 'diverse':\n",
    "            idxs = []\n",
    "            while True:\n",
    "                seen = set()\n",
    "                for idx in range(len(concept_images)):\n",
    "                    if concept_image_numbers[idx] not in seen and idx not in idxs:\n",
    "                        seen.add(concept_image_numbers[idx])\n",
    "                        idxs.append(idx)\n",
    "                if len(idxs) == len(concept_images):\n",
    "                    break\n",
    "        else:\n",
    "            raise ValueError('Invalid mode!')\n",
    "        idxs = idxs[:num]\n",
    "        for i, idx in enumerate(idxs):\n",
    "            ax = plt.Subplot(fig, inner[i])\n",
    "            ax.imshow(concept_images[idx])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            if i == int(num / 2):\n",
    "                ax.set_title(concept)\n",
    "            ax.grid(False)\n",
    "            fig.add_subplot(ax)\n",
    "            ax = plt.Subplot(fig, inner[i + num])\n",
    "            mask = 1 - (np.mean(concept_patches[idx] == float(\n",
    "                cd.average_image_value) / 255, -1) == 1)\n",
    "            image = cd.discovery_images[concept_image_numbers[idx]]\n",
    "            ax.imshow(mark_boundaries(image, mask, color=(1, 1, 0), mode='thick'))\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_title(str(concept_image_numbers[idx]))\n",
    "            ax.grid(False)\n",
    "            fig.add_subplot(ax)\n",
    "    plt.suptitle(bn)\n",
    "    if address is not None:\n",
    "        with tf.gfile.Open(address + bn + '.png', 'w') as f:\n",
    "            fig.savefig(f)\n",
    "        plt.clf()\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Cosine similarity of two vectors.\"\"\"\n",
    "    assert a.shape == b.shape, 'Two vectors must have the same dimensionality'\n",
    "    a_norm, b_norm = np.linalg.norm(a), np.linalg.norm(b)\n",
    "    if a_norm * b_norm == 0:\n",
    "        return 0.\n",
    "    cos_sim = np.sum(a * b) / (a_norm * b_norm)\n",
    "    return cos_sim\n",
    "\n",
    "\n",
    "def similarity(cd, num_random_exp=None, num_workers=25):\n",
    "    \"\"\"Returns cosine similarity of all discovered concepts.\n",
    "\n",
    "  Args:\n",
    "    cd: The ConceptDiscovery module for discovered conceps.\n",
    "    num_random_exp: If None, calculates average similarity using all the class's\n",
    "      random concepts. If a number, uses that many random counterparts.\n",
    "    num_workers: If greater than 0, runs the function in parallel.\n",
    "\n",
    "  Returns:\n",
    "    A similarity dict in the form of {(concept1, concept2):[list of cosine\n",
    "    similarities]}\n",
    "  \"\"\"\n",
    "\n",
    "    def concepts_similarity(cd, concepts, rnd, bn):\n",
    "        \"\"\"Calcualtes the cosine similarity of concept cavs.\n",
    "\n",
    "    This function calculates the pairwise cosine similarity of all concept cavs\n",
    "    versus an specific random concept\n",
    "\n",
    "    Args:\n",
    "      cd: The ConceptDiscovery instance\n",
    "      concepts: List of concepts to calculate similarity for\n",
    "      rnd: a random counterpart\n",
    "      bn: bottleneck layer the concepts belong to\n",
    "\n",
    "    Returns:\n",
    "      A dictionary of cosine similarities in the form of\n",
    "      {(concept1, concept2): [list of cosine similarities], ...}\n",
    "    \"\"\"\n",
    "        similarity_dic = {}\n",
    "        for c1 in concepts:\n",
    "            cav1 = cd.load_cav_direction(c1, rnd, bn)\n",
    "            for c2 in concepts:\n",
    "                if (c1, c2) in similarity_dic.keys():\n",
    "                    continue\n",
    "                cav2 = cd.load_cav_direction(c2, rnd, bn)\n",
    "                similarity_dic[(c1, c2)] = cosine_similarity(cav1, cav2)\n",
    "                similarity_dic[(c2, c1)] = similarity_dic[(c1, c2)]\n",
    "        return similarity_dic\n",
    "\n",
    "    similarity_dic = {bn: {} for bn in cd.bottlenecks}\n",
    "    if num_random_exp is None:\n",
    "        num_random_exp = cd.num_random_exp\n",
    "    randoms = ['random500_{}'.format(i) for i in np.arange(num_random_exp)]\n",
    "    concepts = {}\n",
    "    for bn in cd.bottlenecks:\n",
    "        concepts[bn] = [cd.target_class, cd.random_concept] + cd.dic[bn]['concepts']\n",
    "    for bn in cd.bottlenecks:\n",
    "        concept_pairs = [(c1, c2) for c1 in concepts[bn] for c2 in concepts[bn]]\n",
    "        similarity_dic[bn] = {pair: [] for pair in concept_pairs}\n",
    "\n",
    "        def t_func(rnd):\n",
    "            return concepts_similarity(cd, concepts[bn], rnd, bn)\n",
    "\n",
    "        if num_workers:\n",
    "            pool = multiprocessing.Pool(num_workers)\n",
    "            sims = pool.map(lambda rnd: t_func(rnd), randoms)\n",
    "        else:\n",
    "            sims = [t_func(rnd) for rnd in randoms]\n",
    "        while sims:\n",
    "            sim = sims.pop()\n",
    "            for pair in concept_pairs:\n",
    "                similarity_dic[bn][pair].append(sim[pair])\n",
    "    return similarity_dic\n",
    "\n",
    "\n",
    "def save_ace_report(cd, accs, scores, address):\n",
    "    \"\"\"Saves TCAV scores.\n",
    "\n",
    "  Saves the average CAV accuracies and average TCAV scores of the concepts\n",
    "  discovered in ConceptDiscovery instance.\n",
    "\n",
    "  Args:\n",
    "    cd: The ConceptDiscovery instance.\n",
    "    accs: The cav accuracy dictionary returned by cavs method of the\n",
    "      ConceptDiscovery instance\n",
    "    scores: The tcav score dictionary returned by tcavs method of the\n",
    "      ConceptDiscovery instance\n",
    "    address: The address to save the text file in.\n",
    "  \"\"\"\n",
    "    report = '\\n\\n\\t\\t\\t ---CAV accuracies---'\n",
    "    for bn in cd.bottlenecks:\n",
    "        report += '\\n'\n",
    "        for concept in cd.dic[bn]['concepts']:\n",
    "            report += '\\n' + bn + ':' + concept + ':' + str(\n",
    "                np.mean(accs[bn][concept]))\n",
    "    with tf.gfile.Open(address, 'w') as f:\n",
    "        f.write(report)\n",
    "    report = '\\n\\n\\t\\t\\t ---TCAV scores---'\n",
    "    for bn in cd.bottlenecks:\n",
    "        report += '\\n'\n",
    "        for concept in cd.dic[bn]['concepts']:\n",
    "            pvalue = cd.do_statistical_testings(\n",
    "                scores[bn][concept], scores[bn][cd.random_concept])\n",
    "            report += '\\n{}:{}:{},{}'.format(bn, concept,\n",
    "                                             np.mean(scores[bn][concept]), pvalue)\n",
    "    with tf.gfile.Open(address, 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "\n",
    "def save_concepts(cd, concepts_dir, bs=32):\n",
    "    \"\"\"Saves discovered concept's images or patches.\n",
    "\n",
    "  Args:\n",
    "    cd: The ConceptDiscovery instance the concepts of which we want to save\n",
    "    concepts_dir: The directory to save the concept images\n",
    "  \"\"\"\n",
    "    for bn in cd.bottlenecks:\n",
    "        for concept in cd.dic[bn]['concepts']:\n",
    "            patches_dir = Path(concepts_dir, bn, concept + '_patches')\n",
    "            images_dir = Path(concepts_dir, bn, concept)\n",
    "            \n",
    "            for i in range(int(len(cd.dic[bn][concept]['patches']) / bs) + 1):\n",
    "            \n",
    "                patches = np.array([np.array(Image.open(img)) for img in cd.dic[bn][concept]['patches'][i * bs:(i + 1) * bs]])\n",
    "                # patches = (np.clip(loaded_patches, 0, 1) * 256).astype(np.uint8)\n",
    "\n",
    "                superpixels = np.array([np.array(Image.open(img)) for img in cd.dic[bn][concept]['images'][i * bs:(i + 1) * bs]])\n",
    "                # superpixels = (np.clip(loaded_superpixels, 0, 1) * 256).astype(np.uint8)\n",
    "\n",
    "                Path(patches_dir).mkdir(parents=True, exist_ok=True)\n",
    "                Path(images_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                image_numbers = cd.dic[bn][concept]['image_numbers'][i * bs:(i + 1) * bs]\n",
    "                image_addresses = [images_dir / f\"{img_num}.png\" for img_num in image_numbers]\n",
    "                patch_addresses = [patches_dir / f\"{img_num}.png\" for img_num in image_numbers]\n",
    "                \n",
    "                save_images(patch_addresses, patches)\n",
    "                save_images(image_addresses, superpixels)\n",
    "\n",
    "\n",
    "def save_images(addresses, images):\n",
    "    \"\"\"Save images in the addresses.\n",
    "\n",
    "  Args:\n",
    "    addresses: The list of addresses to save the images as or the address of the\n",
    "      directory to save all images in. (list or str)\n",
    "    images: The list of all images in numpy uint8 format.\n",
    "  \"\"\"\n",
    "    if not isinstance(addresses, list):\n",
    "        image_addresses = []\n",
    "        for i, image in enumerate(images):\n",
    "            image_name = '0' * (3 - int(np.log10(i + 1))) + str(i + 1) + '.png'\n",
    "            image_addresses.append(os.path.join(addresses, image_name))\n",
    "        addresses = image_addresses\n",
    "    assert len(addresses) == len(images), 'Invalid number of addresses'\n",
    "    for address, image in zip(addresses, images):\n",
    "        Image.fromarray(image).save(address, format='PNG')\n",
    "\n",
    "def ceildiv(a, b):\n",
    "    return -(a // -b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8992b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file Utils/TCAV/tcav.py\n",
    "\n",
    "import numpy as np\n",
    "from Utils.TCAV.cav import CAV\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def directional_derivative(model, cav, layer_name, class_name):\n",
    "    gradient = model.generate_gradients(class_name, layer_name).reshape(-1)\n",
    "    return np.dot(gradient, cav) < 0\n",
    "\n",
    "\n",
    "def tcav_score(model, data_loader, cav, layer_name, class_list, concept):\n",
    "    derivatives = {}\n",
    "    for k in class_list:\n",
    "        derivatives[k] = []\n",
    "\n",
    "    tcav_bar = tqdm(data_loader)\n",
    "    tcav_bar.set_description('Calculating tcav score for %s' % concept)\n",
    "    for x, _ in tcav_bar:\n",
    "        model.eval()\n",
    "        x = x.to(device)\n",
    "        outputs = model(x)\n",
    "        k = int(outputs.max(dim=1)[1].cpu().detach().numpy())\n",
    "        if k in class_list:\n",
    "            derivatives[k].append(directional_derivative(model, cav, layer_name, k))\n",
    "\n",
    "    score = np.zeros(len(class_list))\n",
    "    for i, k in enumerate(class_list):\n",
    "        score[i] = np.array(derivatives[k]).astype(np.int).sum(axis=0) / len(derivatives[k])\n",
    "    return score\n",
    "\n",
    "\n",
    "class TCAV(object):\n",
    "    def __init__(self, model, input_dataloader, concept_dataloaders, class_list, max_samples):\n",
    "        self.model = model\n",
    "        self.input_dataloader = input_dataloader\n",
    "        self.concept_dataloaders = concept_dataloaders\n",
    "        self.concepts = list(concept_dataloaders.keys())\n",
    "        self.output_dir = 'output'\n",
    "        self.max_samples = max_samples\n",
    "        self.lr = 1e-3\n",
    "        self.model_type = 'logistic'\n",
    "        self.class_list = class_list\n",
    "\n",
    "    def generate_cavs(self, layer_name):\n",
    "        cav_trainer = CAV(self.concepts, layer_name, self.lr, self.model_type)\n",
    "        cav_trainer.train(self.activations)\n",
    "        self.cavs = cav_trainer.get_cav()\n",
    "\n",
    "    def calculate_tcav_score(self, layer_name, output_path):\n",
    "        self.scores = np.zeros((self.cavs.shape[0], len(self.class_list)))\n",
    "        for i, cav in enumerate(self.cavs):\n",
    "            self.scores[i] = tcav_score(self.model, self.input_dataloader, cav, layer_name, self.class_list,\n",
    "                                        self.concepts[i])\n",
    "        # print(self.scores)\n",
    "        np.save(output_path, self.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03bbce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file Utils/TCAV/cav.py\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def flatten_activations_and_get_labels(concepts, layer_name, activations):\n",
    "    '''\n",
    "    :param concepts: different name of concepts\n",
    "    :param layer_name: the name of the layer to compute CAV on\n",
    "    :param activations: activations with the size of num_concepts * num_layers * num_samples\n",
    "    :return:\n",
    "    '''\n",
    "    # in case of different number of samples for each concept\n",
    "    min_num_samples = np.min([activations[c][layer_name].shape[0] for c in concepts])\n",
    "    # flatten the activations and mark the concept label\n",
    "    data = []\n",
    "    concept_labels = np.zeros(len(concepts) * min_num_samples)\n",
    "    label_concept = {}\n",
    "    for i, c in enumerate(concepts):\n",
    "        data.extend(activations[c][layer_name][:min_num_samples].reshape(min_num_samples, -1))\n",
    "        concept_labels[i * min_num_samples : (i + 1) * min_num_samples] = i\n",
    "        label_concept[i] = c\n",
    "    data = np.array(data)\n",
    "    return data, concept_labels, label_concept\n",
    "\n",
    "\n",
    "class CAV(object):\n",
    "    def __init__(self, concepts, layer_name, save_path, hparams=None):\n",
    "        self.concepts = concepts\n",
    "        self.layer_name = layer_name\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        if hparams:\n",
    "            self.hparams = hparams\n",
    "        else:\n",
    "            self.hparams = {'model_type':'linear', 'alpha':.01}\n",
    "    \n",
    "    def cav_filename(self):\n",
    "        concepts = \"_\".join([str(c) for c in self.concepts])\n",
    "        model_type = self.hparams[\"model_type\"]\n",
    "        alpha = self.hparams[\"alpha\"]\n",
    "        \n",
    "        return f\"{concepts}_{self.layer_name}_{model_type}_{alpha}.pkl\"\n",
    "\n",
    "    def train(self, activations):\n",
    "        data, labels, label_concept = flatten_activations_and_get_labels(self.concepts, self.layer_name, activations)\n",
    "\n",
    "        # default setting is One-Vs-All\n",
    "        assert self.hparams[\"model_type\"] in ['linear', 'logistic']\n",
    "        if self.hparams[\"model_type\"] == 'linear':\n",
    "            model = SGDClassifier(alpha=self.hparams[\"alpha\"])\n",
    "        else:\n",
    "            model = LogisticRegression()\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, stratify=labels)\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        # Get accuracy\n",
    "        y_pred = model.predict(x_test)\n",
    "        # get acc for each class.\n",
    "        num_classes = max(labels) + 1\n",
    "        acc = {}\n",
    "        num_correct = 0\n",
    "        \n",
    "        for class_id in range(int(num_classes)):\n",
    "            \n",
    "            # get indices of all test data that has this class.\n",
    "            idx = (y_test == class_id)\n",
    "            \n",
    "            acc[label_concept[class_id]] = metrics.accuracy_score(y_pred[idx], y_test[idx])\n",
    "\n",
    "              # overall correctness is weighted by the number of examples in this class.\n",
    "            num_correct += (sum(idx) * acc[label_concept[class_id]])\n",
    "            \n",
    "        acc['overall'] = float(num_correct) / float(len(y_test))\n",
    "            \n",
    "        self.accuracies = acc\n",
    "        \n",
    "        '''\n",
    "        The coef_ attribute is the coefficients in linear regression.\n",
    "        Suppose y = w0 + w1x1 + w2x2 + ... + wnxn\n",
    "        Then coef_ = (w0, w1, w2, ..., wn). \n",
    "        This is exactly the normal vector for the decision hyperplane\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        if len(model.coef_) == 1:\n",
    "            self.cavs = np.array([-model.coef_[0], model.coef_[0]])\n",
    "        else:\n",
    "            self.cavs = -np.array(model.coef_)\n",
    "        \n",
    "        self.save_cav()\n",
    "\n",
    "    def get_cav(self):\n",
    "        return self.cav\n",
    "    \n",
    "    def save_cav(self):\n",
    "        \"\"\"Save a dictionary of this CAV to a pickle.\"\"\"\n",
    "        \n",
    "        save_dict = {\n",
    "            'concepts': self.concepts,\n",
    "            'bottleneck': self.layer_name,\n",
    "            'hparams': self.hparams,\n",
    "            'accuracies': self.accuracies,\n",
    "            'cavs': self.cavs,\n",
    "            'saved_path': self.save_path\n",
    "            }\n",
    "        \n",
    "        if self.save_path is not None:\n",
    "            with open(self.save_path / self.cav_filename(), 'wb') as pkl_file:\n",
    "                pickle.dump(save_dict, pkl_file)\n",
    "    \n",
    "    def load_cav(cav_path):\n",
    "        \"\"\"Make a CAV instance from a saved CAV (pickle file).\n",
    "            Args:\n",
    "            cav_path: the location of the saved CAV\n",
    "            Returns:\n",
    "            CAV instance.\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(cav_path, 'rb') as pkl_file:\n",
    "            save_dict = pickle.load(pkl_file)\n",
    "\n",
    "        cav = CAV(save_dict['concepts'], save_dict['bottleneck'], \n",
    "                  save_dict['hparams'], save_dict['saved_path'])\n",
    "        \n",
    "        cav.accuracies = save_dict['accuracies']\n",
    "        cav.cavs = save_dict['cavs']\n",
    "        return cav\n",
    "\n",
    "def load_or_train_cav(concepts, layer_name, save_path, hparams=None, activations=None, overwrite=False):\n",
    "    cav_instance = CAV(concepts, layer_name, save_path, hparams)\n",
    "    \n",
    "    if save_path is not None:\n",
    "        cav_path = Path(save_path) / cav_instance.cav_filename()\n",
    "        \n",
    "    if not overwrite and cav_path.isfile():\n",
    "        cav_instance = CAV.load_cav(cav_path)\n",
    "        return cav_instance\n",
    "\n",
    "    if activations is not None:\n",
    "        cav_instance.train(activations)\n",
    "        return cav_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b211842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6431fe90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
