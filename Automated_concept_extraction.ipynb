{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbfd106",
   "metadata": {},
   "source": [
    "### Automated Concept Extraction\n",
    "\n",
    "There are several steps to the automated concept extraction method that are outlined in their paper [here]().\n",
    "\n",
    "Firstly, we need to create patches from images that represent the object we want to derive concepts for. This involves using skimage segmentation and extracting the patches and superpixels from this. These will be used to find visual features that can be used as concepts.\n",
    "\n",
    "Once we have the superpixels that we need, we can make use of a clustering technique on the representations extracted from the bottleneck layers of our model after passing a superpixel through. This will allow us to find visually similar images that will hopefully group patches that represent the same visual concept.\n",
    "\n",
    "These groups of superpixels can then be used to create a concept activation vector. This involves getting the activations of these superpixels relating to a concept and then random superpixels that as a group represent no descernable concept. A linear classifier is trained on these examples and the vector orthognal to the hyperplane that separates the concept examples from the random is taken to be the concept activation vector.\n",
    "\n",
    "The influence of this concept can be determined by taking the partial derivative of the class logit you want to examine with respect to a bottleneck layer. Multiplying the CAV by this partial derivative will allow us to determine the impact this concept had on the prediction.\n",
    "\n",
    "This concludes the rough overview of the method that will be employed. The aim is to create realistic, reasonable concepts from the mitotic figures without the need of manually gathering images of specific concepts.\n",
    "\n",
    "### Importing libraries\n",
    "\n",
    "We will be making use of the following libraries to implement our method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "604bb091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "from PIL import Image\n",
    "\n",
    "import Utils.ACE.ace_helpers as ace_helpers\n",
    "from Utils.ACE.ace import ConceptDiscovery\n",
    "from Utils.ACE.run_ace import run_concept_discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc5132",
   "metadata": {},
   "source": [
    "### Testing with COCO\n",
    "\n",
    "We will begin by taking some images from the COCO dataset, specifically those inclusing a tennis racket. We will use this example to test our method and ensure we are processing the images correctly and the results seem reasonable. This seems the best course of action as I have a better understanding of the visual features that concern a tennis racket and no formal understanding of the visual features of a mitotic figure.\n",
    "\n",
    "I believe that using the COCO images may result in poor concepts as the images are not specifically tennis rackets, but instead images that contain tennis rackets. This means that we will get many superpixels that are not relevant to tennis rackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88368f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output directory for our data\n",
    "output = Path.cwd() / \"ACE_COCO_output/\"\n",
    "\n",
    "# Create the sub directories at the output location.\n",
    "ace_helpers.create_directories(output, remove_old=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1144ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the target class and the source directory.\n",
    "# Note: The target class should be a folder in the source directory.\n",
    "# This folder should have a folder called discovery for images for concept discovery and a folder called tcav for tcav score calculation.\n",
    "target_class = \"tennis racket\"\n",
    "source_dir = \"D:\\DS\\DS4\\Project\\COCO\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484f0a8b",
   "metadata": {},
   "source": [
    "We have defined the output path and the source directory where our images for discovery and tcav scores are stored.\n",
    "\n",
    "Now we need to load in our model, this is wrapped in the ModelWrapper class which will extract the activations and gradients for us so we can cluster the superpixels, create cavs adn get tcav scores.\n",
    "\n",
    "### Selecting the bottleneck layers\n",
    "\n",
    "In order to extract the activations and gradients from a layer, we need to determine which layer(s) are bottleneck layers. A bottleneck layer typically reduces the number of channels in the data between the input and output while keeping the size of the image equal by using a kernel of (1,1) and a stride of (1,1). This means that the model compresses the representation of the input in this layer and keeps the most important features for performing the task. This makes it the ideal layer for using the activations from to cluster the superpixels for ACE and to train the linear classifier to find a CAV.\n",
    "\n",
    "Looking at the model structure below we can see that there are several such layers in the backbone of our model. It may be worth just taking a selection of these. I have decided to take the bottleneck from the last bottleneck unit in each layer. This means I will be using the following 4 layers.\n",
    "\n",
    "```\n",
    "bottleneck_layers = ['backbone.body.layer1.2.conv1', 'backbone.body.layer2.3.conv1', 'backbone.body.layer3.5.conv1', 'backbone.body.layer4.2.conv1']\n",
    "```\n",
    "\n",
    "These will be the layers I extract both the activations from and the gradients when looking at the influence of each concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "756225f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): FastRCNNConvFCHead(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Flatten(start_dim=1, end_dim=-1)\n",
       "      (5): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of the bottleneck layers.\n",
    "bottleneck_layers = ['backbone.body.layer1.2.conv1', 'backbone.body.layer2.3.conv1', 'backbone.body.layer3.5.conv1', 'backbone.body.layer4.2.conv1']\n",
    "\n",
    "# Create the model variable and set it to evaluate.\n",
    "mymodel = ace_helpers.MyModel(\"tmp\", bottleneck_layers)\n",
    "mymodel.model.eval()\n",
    "mymodel.model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29fbe8",
   "metadata": {},
   "source": [
    "### ConceptDiscovery class\n",
    "\n",
    "Now that we have all of the required parameters, we can initialize the ConceptDiscovery class, which contains the methods for creating superpixels, clustering to find concepts, creating concept activation vectors and testing these. We will make use of this class for most of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4384fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the ConceptDiscovery class instance.\n",
    "cd = ConceptDiscovery(\n",
    "    mymodel,\n",
    "    target_class,\n",
    "    source_dir,\n",
    "    output,\n",
    "    bottleneck_layers,\n",
    "    num_random_exp=2,\n",
    "    channel_mean=True,\n",
    "    max_imgs=10,\n",
    "    min_imgs=5,\n",
    "    num_discovery_imgs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d20ea",
   "metadata": {},
   "source": [
    "Now we can call create_patches. This function takes the discovery images in the source directory and segments the image into superpixels for use later. The number of segments can be specified below.\n",
    "\n",
    "These superpixels are then saved in the concept subdirectory of the output along with the patches and the discovery images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b4ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset of image patches.\n",
    "cd.create_patches(param_dict={'n_segments': [15]})\n",
    "\n",
    "# Saving the concept discovery target class images.\n",
    "image_dir = cd.discovered_concepts_dir / 'images'\n",
    "image_dir.mkdir()\n",
    "ace_helpers.save_images(image_dir.absolute(),\n",
    "                        (cd.discovery_images * 256).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244110c9",
   "metadata": {},
   "source": [
    "The discover_concepts function takes all of the superpixels that were produced and determines possible concepts. This is done by passing the superpixels through the model and extracting the activations in the bottleneck layers. These representations of the superpixels can be clustered to find similar superpixels, which can be considered to be a possible visual concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5c2cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Discovering Concepts\n",
    "cd.discover_concepts(method='KM', param_dicts={'n_clusters': 25})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc9f8d6",
   "metadata": {},
   "source": [
    "Now we can save these discovered concepts to the concept folder for visual review and use later on when generating concept activation vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad517a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save discovered concept images (resized and original sized)\n",
    "ace_helpers.save_concepts(cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f9ff91",
   "metadata": {},
   "source": [
    "In order to test the validity of our findings, it is important to compare our CAV against a random concept. In addition, I will carry out a two-sided t-test in order to statistically quantify that the resultant scores are significantly different from one another. \n",
    "\n",
    "The initialize_random_concept_and_samples method creates a folder called Random in the concept directory and randomly samples superpixels for a random concept and a number of random samples for use as a random counterpart for CAV generation. This will allow us to determine that the found CAV from our clustered superpixels can explain the prediction better than a random selection, helping to qualify the findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd7a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.initialize_random_concept_and_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7df633",
   "metadata": {},
   "source": [
    "The cavs method takes the superpixels from each of the concept directories and a random counterpart and finds the resultant CAV. A CAV will be generated for every pair of concept and random counterpart. If we are carrying out N experiments and we have M concepts discovered we will have N * (M + 1) CAVs, as we have to include the random concept with the M found potential concepts from clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe6ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cav_accuracies = cd.cavs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55683156",
   "metadata": {},
   "source": [
    "We can see the returned accuracies for each CAV generated (The experiments are listed). This helps us to understand how well the linear classifier splits the potential concepts superpixels from the randomly selected superpixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09d1291",
   "metadata": {},
   "outputs": [],
   "source": [
    "cav_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f921002a",
   "metadata": {},
   "source": [
    "Now we can get a score of how influencial the individual CAVs are on all of the predictions on images from the tcav folder within the source directory.\n",
    "\n",
    "This tcavs method takes the images from the tcav folder and calculates the gradients for the class of interest with respect to the bottleneck layers. These gradients are then multiplied by the CAV vector to determine how relevant the CAV is in the prediction. This is results in a list of scores for each concept, an entry for each random counterpart/ experiment.\n",
    "\n",
    "These scores are then used in a two-sided t-test with the random concept to determine if the potential concept is statistically different than a random selection of the superpixels. This will aid in validiating any findings from the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac7672",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cd.tcavs(test=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81989e7",
   "metadata": {},
   "source": [
    "The CAV accuracies and the TCAV scores can be saved into a report text file for review. This includes the average accuracy for a CAV, average tcav score for a CAV and the p-value from the t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.save_ace_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b1f19",
   "metadata": {},
   "source": [
    "We can look at the superpixels that make up a concept to visually identify any characteristics that may have been used to cluster these superpixels together to find a potential concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c7a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot examples of discovered concepts\n",
    "for bn in cd.bottlenecks:\n",
    "    cd.plot_concepts(bn, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac79167",
   "metadata": {},
   "source": [
    "### Dimensionality reduction for clustering\n",
    "\n",
    "The current implementation is averaging the activation values across the filters to reduce the dimensionality and allow the clustering to be performed. This is not the best approach to reduce the dimensionality while preserving as much information as possible. Improving this method will hopefully lead to more meaningful potential concepts.\n",
    "\n",
    "In order the pick the best method for this application it would be best to understand the sparsity of the data we intend to run the dimensionality reduction on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2722d17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the channel mean to false\n",
    "cd.channel_mean == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75fbf22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating activations for superpixels: 100%|█████████████████████████████████████████| 63/63 [00:54<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the superpixel images\n",
    "superpixels_dir = cd.discovered_concepts_dir / \"superpixels\"\n",
    "superpixel_images = np.array(list(superpixels_dir.iterdir()))\n",
    "\n",
    "# Get the activations back after passing the superpixels.\n",
    "activations = cd._get_activations(superpixel_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a214a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density of backbone.body.layer1.2.conv1 is 1.0\n",
      "Density of backbone.body.layer2.3.conv1 is 1.0\n",
      "Density of backbone.body.layer3.5.conv1 is 1.0\n",
      "Density of backbone.body.layer4.2.conv1 is 1.0\n"
     ]
    }
   ],
   "source": [
    "# For every bottleneck layer and their activations.\n",
    "for bn, v in activations.items():\n",
    "    \n",
    "    # Get the count of nonzero values\n",
    "    non_zero = np.count_nonzero(v)\n",
    "    \n",
    "    # Get the total number of entries in the array\n",
    "    total = np.prod(v.shape)\n",
    "    \n",
    "    # The density is the non_zero count divided by the total entries.\n",
    "    density = non_zero / total\n",
    "    \n",
    "    # Print the density for each.\n",
    "    print(f\"Density of {bn} is {density}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdf9fb",
   "metadata": {},
   "source": [
    "We can see that there seems to be no zeroes present in any of the activation matrices. We can conform this by counting the values that are zero in the matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4544548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of zeroes in backbone.body.layer1.2.conv1 is 0\n",
      "The number of zeroes in backbone.body.layer2.3.conv1 is 0\n",
      "The number of zeroes in backbone.body.layer3.5.conv1 is 0\n",
      "The number of zeroes in backbone.body.layer4.2.conv1 is 0\n"
     ]
    }
   ],
   "source": [
    "# For every bottleneck layer and their activations.\n",
    "for bn, v in activations.items():\n",
    "    \n",
    "    # Get the sum of boolean values when array is == 0 i.e. the count of values that are 0\n",
    "    zeroes = np.sum(v == 0)\n",
    "    \n",
    "    # Print the number of zeroes.\n",
    "    print(f\"The number of zeroes in {bn} is {zeroes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83623e76",
   "metadata": {},
   "source": [
    "This means that we have dense arrays and it would be best to use principal component analysis PCA. This can be implemented through sci-kit learn with ease. Let us test this and see how much of the variance is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6ffb442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29296002 0.35472158 0.3988791  0.43858668 0.46609247 0.49220186\n",
      " 0.51350254 0.53409564 0.5538933  0.5727218  0.58922905 0.6032458\n",
      " 0.6161591  0.6283241  0.63969785 0.6505502  0.6613581  0.6707555\n",
      " 0.67992985 0.6887615  0.69704604 0.705123   0.71298987 0.72023004\n",
      " 0.7272062  0.73397005 0.740381   0.746599   0.7526822  0.7586139\n",
      " 0.76436627 0.76993877 0.77545404 0.7808514  0.7860959  0.7913027\n",
      " 0.7964275  0.8014505  0.80632865 0.81113684 0.8157745  0.8203254\n",
      " 0.82477874 0.82917327 0.8334361  0.83766514 0.8417832  0.8458774\n",
      " 0.849889   0.8537952  0.85760325 0.8613863  0.86504877 0.8687017\n",
      " 0.8722755  0.87572306 0.8791323  0.8824883  0.88577646 0.88897413\n",
      " 0.892126   0.89523333 0.89825493 0.9012536  0.9041931  0.9070981\n",
      " 0.90993416 0.91273284 0.9154793  0.9181698  0.9207798  0.9233103\n",
      " 0.92578477 0.9282323  0.93064463 0.9329986  0.93533844 0.93753713\n",
      " 0.9396882  0.94180286 0.9438995  0.94597274 0.94796187 0.94991916\n",
      " 0.9518593  0.9537623  0.95560545 0.95740646 0.9592006  0.96095145\n",
      " 0.9626924  0.9643953  0.9660777  0.9677513  0.96936953 0.97096515\n",
      " 0.97255903 0.97408587 0.97556263 0.9769707  0.9783562  0.9797179\n",
      " 0.98104656 0.9823378  0.98361677 0.98484045 0.9860607  0.9872741\n",
      " 0.9884523  0.98959696 0.9907282 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set up the PCA model, preserve 99% of the variance\n",
    "my_model = PCA(n_components= 0.99)\n",
    "\n",
    "# Fit the data to the model and return the transformed data.\n",
    "results = my_model.fit_transform(activations[\"backbone.body.layer1.2.conv1\"])\n",
    "\n",
    "# Print the cumulative sum of the variance ratios to understand how much variance is preserved after adding each variable.\n",
    "print(my_model.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9b27f0",
   "metadata": {},
   "source": [
    "We can see that our PCA has complete and we have been returned the transformed data. Below we can look at the shape before and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87a1d0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126, 2560000)\n",
      "(126, 111)\n"
     ]
    }
   ],
   "source": [
    "# Print the old shape\n",
    "print(activations[\"backbone.body.layer1.2.conv1\"].shape)\n",
    "\n",
    "# Print the new shape\n",
    "print(results.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eec715",
   "metadata": {},
   "source": [
    "The largest amount of features in the new space is the minimum between the number of samples and the number of features. This seems to work well, though this test set has only 126 activations for the superpixels coming from 10 images. The dataset that we have contains several tissue types, and the one that contains the most annotations has ~ 4,000 mitotic figures. This means that if we have roughly 10 segments per image, that will equate to ~ 40,000 activations being stored. This is not a feasible number to hold in memory. Below we determine the memnory usage for the current toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f389bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.body.layer1.2.conv1 size of array is 1230.47MB\n",
      "backbone.body.layer2.3.conv1 size of array is 615.23MB\n",
      "backbone.body.layer3.5.conv1 size of array is 307.62MB\n",
      "backbone.body.layer4.2.conv1 size of array is 153.81MB\n"
     ]
    }
   ],
   "source": [
    "# For every bottleneck layer and their activations.\n",
    "for bn, v in activations.items():\n",
    "    \n",
    "    # Get the array size in bytes and convert to MB.\n",
    "    array_size = round(v.nbytes / 1024 / 1024,2)\n",
    "    \n",
    "    print(f\"{bn} size of array is {array_size}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60418c9b",
   "metadata": {},
   "source": [
    "For 126 activations we have a memory usage of 1.23GB, ~ 10MB per image. So for the dataset we intend to use the memory usage would be 10MB * 40,000 = 400,000MB = 400GB. Even when we were averaging over the filters we were reducing the memory usage for each image by a factor of 64, so the usage would be ~6.25GB for the first layer, ~3GB for the second, ~1.5GB for the third and ~0.75GB for the last. So regardless, we need a method to reduce the dimensionality.\n",
    "\n",
    "Thankfully, there is a PCA implementation for processing data in batches, which will allow us to process the activations one batch at a time and never have to hold them all in memeory. This will also result in better clusters as more information is preserved.\n",
    "\n",
    "### Testing incremental PCA \n",
    "\n",
    "I will test how well incremental PCA works and also determine if it is feasible to run it on my machine. I will make use of the largest tissue group to ensure that it will work for the rest which are smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8c7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89ea4a13",
   "metadata": {},
   "source": [
    "This concludes our test implementation of the method. We will now sanity check several components of the code to ensure that there are no issues and components are working as expected.\n",
    "\n",
    "### Sanity check\n",
    "\n",
    "To ensure that the implementation is working as I expect it to I will check the following:\n",
    "\n",
    "That the activations are equivalent both individually and in batches.\n",
    "\n",
    "To check this I will manually run the activations both individually and in a batch and ensure the result is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a90585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in some test files for passing to get activations\n",
    "test_files = np.array(list((cd.discovered_concepts_dir / \"superpixels\").iterdir())[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be88e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the activations with a batch size of 1.\n",
    "acts = cd._get_activations(test_files, bs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69399b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gradients with a batch size of 2.\n",
    "batch_acts = cd._get_activations(test_files, bs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab21c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the activations are equal.\n",
    "np.unique(acts[\"backbone.body.layer1.2.conv1\"] == batch_acts[\"backbone.body.layer1.2.conv1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f9851",
   "metadata": {},
   "source": [
    "We can see that the activations are equivalent, so we have checked this.\n",
    "\n",
    "Next, I want to check that the hooks don't require the gradients to be zeroed and are not being accumulated.\n",
    "\n",
    "This will involve getting the returned gradients, and ensuring that every element in the list is not smaller than the next element in the list. If the gradients were accummulating, this would be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865b16dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gradients for our test images.\n",
    "grads, _ = cd._return_gradients(test_files, test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f5d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that every value in gradient i-1 in the list is not smaller than in gradient i.\n",
    "layer_grads = grads[\"backbone.body.layer1.2.conv1\"]\n",
    "for i in range(1, len(layer_grads)):\n",
    "    print(np.unique(layer_grads[i - 1] <= layer_grads[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6413cf0d",
   "metadata": {},
   "source": [
    "We can see that there are instances where this is false, so the gradient in gradient i-1 is greater than in gradient i at some values. This shows that the gradients are not being accummulated.\n",
    "\n",
    "Lastly, I want to ensure that the method of calculating the TCAV score is working as expected and producing correct scores. This will involve manually running the method and ensuring the output is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244afc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some gradients to use\n",
    "gradients, _ = cd._return_gradients(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff29116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd._tcav_score(\"backbone.body.layer1.2.conv1\", \"tennis racket_concept1\", \"Random_001\", gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a108d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the CAV vector.\n",
    "tmp_vector = cd.load_cav_direction(\"tennis racket_concept1\", \"Random_001\", \"backbone.body.layer1.2.conv1\")\n",
    "\n",
    "# Check the shape of the vector.\n",
    "tmp_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b986d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get these test gradients.\n",
    "test_grads = gradients[\"backbone.body.layer1.2.conv1\"]\n",
    "\n",
    "# Check the test gradients shape.\n",
    "len(test_grads), len(test_grads[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c1cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply the vector against the gradient.\n",
    "prod = (test_grads * vector)\n",
    "prod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fca8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sum along axis 1 so we have the dot products.\n",
    "dot_products = np.sum(prod, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f7a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average value of the booleans returned.\n",
    "np.mean(dot_products < 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46eede6",
   "metadata": {},
   "source": [
    "We can see that the resultant score is identical for both. The manual method I have used follows the original implementation of the tcav score in the paper.\n",
    "\n",
    "### ACE on Mitotic figures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output directory for our data\n",
    "output = Path.cwd() / \"ACE_mitotic_output/\"\n",
    "\n",
    "target_class = \"mitotic figure\"\n",
    "source_dir = \"D:\\DS\\DS4\\Project\\MIDOG\"\n",
    "\n",
    "bottleneck_layers = ['backbone.body.layer1.2.conv1', 'backbone.body.layer2.3.conv1', 'backbone.body.layer3.5.conv1', 'backbone.body.layer4.2.conv1']\n",
    "\n",
    "model_params = {\"model\": \"mitotic\", \"layers\": bottleneck_layers}\n",
    "\n",
    "cd_params = {\"bottlenecks\": bottleneck_layers, \"num_random_exp\": 100, \"channel_mean\": True, \"max_imgs\": 40, \"min_imgs\": 20, \"num_discovery_imgs\": 40, \"average_image_value\": 117, \"resize_dims\": (512, 512)}\n",
    "\n",
    "patch_params = {\"method\": 'slic', \"param_dict\": {'n_segments': [15]}}\n",
    "\n",
    "clustering_params = {\"method\": 'KM', \"param_dicts\": {'n_clusters': 25}}\n",
    "\n",
    "cav_params = {\"min_acc\": 0.5}\n",
    "\n",
    "tcav_params = {\"test\": False, \"sort\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc3838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_concept_discovery(output, target_class, source_dir, model_params, cd_params, patch_params, clustering_params, cav_params, tcav_params)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856991de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the ConceptDiscovery class instance.\n",
    "cd = ConceptDiscovery(\n",
    "    mymodel,\n",
    "    target_class,\n",
    "    random_concept,\n",
    "    ['backbone.body.layer1.2.conv1', 'backbone.body.layer2.3.conv1', 'backbone.body.layer3.5.conv1', 'backbone.body.layer4.2.conv1'],\n",
    "    source_dir,\n",
    "    activations_dir,\n",
    "    cavs_dir,\n",
    "    num_random_exp=2,\n",
    "    channel_mean=True,\n",
    "    max_imgs=100,\n",
    "    min_imgs=50,\n",
    "    num_discovery_imgs=100,\n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab48702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset of image patches.\n",
    "cd.create_patches(discovered_concepts_dir, param_dict={'n_segments': [15]})\n",
    "\n",
    "# Saving the concept discovery target class images.\n",
    "image_dir = discovered_concepts_dir / 'images'\n",
    "image_dir.mkdir()\n",
    "ace_helpers.save_images(image_dir.absolute(),\n",
    "                        (cd.discovery_images * 256).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe90edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discovering Concepts\n",
    "cd.discover_concepts(discovered_concepts_dir, method='KM', param_dicts={'n_clusters': 25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c19c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save discovered concept images (resized and original sized)\n",
    "ace_helpers.save_concepts(cd, discovered_concepts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3eb43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add to helper function generate_random\n",
    "superpixels = discovered_concepts_dir / \"superpixels\"\n",
    "list_of_files = list(superpixels.iterdir())\n",
    "\n",
    "# Random selection of the the superpixels for random concept?\n",
    "random.seed(42)\n",
    "cd.random_imgs = np.array(random.sample(list_of_files, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff147b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the random imgs for review save_random\n",
    "for img in cd.random_imgs:\n",
    "    destination = img.parent.parent / \"Random\"\n",
    "    destination.mkdir(exist_ok=True)\n",
    "    \n",
    "    shutil.copy(img, destination / img.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778eff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "cav_accuracies = cd.cavs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe081e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
