{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbfd106",
   "metadata": {},
   "source": [
    "### Automated Concept Extraction\n",
    "\n",
    "There are several steps to the automated concept extraction method that are outlined in their paper [here]().\n",
    "\n",
    "Firstly, we need to create patches from images that represent the object we want to derive concepts for. This involves using skimage segmentation and extracting the patches and superpixels from this. These will be used to find visual features that can be used as concepts.\n",
    "\n",
    "Once we have the patches that we need, we can make use of a clustering technique on the representations extracted from the bottleneck layers of our model after passing a patch through. This will allow us to find visually similar images that will hopefully group patches that represent the same visual concept.\n",
    "\n",
    "These groups of patches can then be used to create a concept activation vector. This involves getting the activations of these ptaches relating to a concept and then random patches that as a group represent no descernable concept. A linear classifier is trained on these examples and the vector orthognal to the hyperplane that separates the concept examples from the random is taken to be the concept activation vector.\n",
    "\n",
    "The influence of this concept can be determined by taking the partial derivative of the class logit you want to examine with respect to a bottleneck layer. Multiplying the CAV by this partial derivative will allow us to determine the impact this concept had on the prediction.\n",
    "\n",
    "This concludes the rough overview of the method that will be employed. The aim is to create realistic, reasonable concepts from the mitotic figures without the need of manually gathering images of specific concepts.\n",
    "\n",
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "604bb091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "# from tcav import utils\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "import Utils.ACE.ace_helpers as ace_helpers\n",
    "from Utils.ACE.ace import ConceptDiscovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc5132",
   "metadata": {},
   "source": [
    "### Testing with COCO\n",
    "\n",
    "We will begin by taking some images from the COCO dataset, specifically those inclusing a tennis racket. We will use this example to test our method and ensure we are processing the images correctly and the results seem reasonable. This seems the best course of action as I have a better understanding of the visual features that concern a tennis racket and no formal understanding of the visual features of a mitotic figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88368f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output directory for our data\n",
    "output = Path.cwd() / \"ACE_COCO_output/\"\n",
    "\n",
    "# Create the relevant sub-directories.\n",
    "discovered_concepts_dir = output / 'concepts/'\n",
    "results_dir = output / 'results/'\n",
    "cavs_dir = output / 'cavs/'\n",
    "activations_dir = output / 'acts/'\n",
    "results_summaries_dir = output / 'results_summaries/'\n",
    "\n",
    "# If the directory exists we delete it to generate new output.\n",
    "if output.exists():\n",
    "    shutil.rmtree(output)\n",
    "\n",
    "# Make all of the directories\n",
    "output.mkdir()\n",
    "discovered_concepts_dir.mkdir()\n",
    "results_dir.mkdir()\n",
    "cavs_dir.mkdir()\n",
    "activations_dir.mkdir()\n",
    "results_summaries_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1144ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the target class and the source directory.\n",
    "target_class = \"tennis racket\"\n",
    "source_dir = \"D:\\DS\\DS4\\Project\\COCO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "756225f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Random concept for statistical testing.\n",
    "random_concept = 'random_discovery'\n",
    "\n",
    "# Define the bottleneck layers.\n",
    "\n",
    "# Create the model variable and set it to evaluate.\n",
    "mymodel = ace_helpers.MyModel(\"tmp\", ['backbone.body.layer1.2.conv1', 'backbone.body.layer2.3.conv1', 'backbone.body.layer3.5.conv1', 'backbone.body.layer4.2.conv1'])\n",
    "mymodel.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b055361b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): FastRCNNConvFCHead(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Flatten(start_dim=1, end_dim=-1)\n",
       "      (5): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71e58d",
   "metadata": {},
   "source": [
    "### Selecting the bottleneck layers\n",
    "\n",
    "In order to extract the activations and gradients from a layer, we need to determine which layer(s) are bottleneck layers. A bottleneck layer typically reduces the number of channels in the data between the input and output while keeping the size of the image equal by using a kernel of (1,1) and a stride of (1,1). This means that the model compresses the representation of the input in this layer and keeps the most important features for performing the task. This makes it the ideal layer for using the activations from to cluster the patches for ACE and to train the linear classifier for TCAV.\n",
    "\n",
    "Looking at the model structure from above we can see that there are several such layers in the backbone of our model. It may be worth just taking a selection of these. I have decided to take the bottleneck from the last bottleneck unit in each layer. This means I will be using the following 4 layers.\n",
    "\n",
    "```\n",
    "bottleneck_layers = ['backbone.body.layer1.2.conv1', 'backbone.body.layer2.3.conv1', 'backbone.body.layer3.5.conv1', 'backbone.body.layer4.2.conv1']\n",
    "```\n",
    "\n",
    "These will be the layers I extract both the activations from and the gradients when looking at the influence of each concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4384fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the ConceptDiscovery class instance.\n",
    "cd = ConceptDiscovery(\n",
    "    mymodel,\n",
    "    target_class,\n",
    "    random_concept,\n",
    "    ['backbone.body.layer1.2.conv1', 'backbone.body.layer2.3.conv1', 'backbone.body.layer3.5.conv1', 'backbone.body.layer4.2.conv1'],\n",
    "    source_dir,\n",
    "    activations_dir,\n",
    "    cavs_dir,\n",
    "    num_random_exp=2,\n",
    "    channel_mean=True,\n",
    "    max_imgs=10,\n",
    "    min_imgs=5,\n",
    "    num_discovery_imgs=10,\n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa5f22",
   "metadata": {},
   "source": [
    "We have initialized the ConceptDiscovery class, which contains the methods for creating superpixels, clustering to find concepts, creating concept activation vectors and testing these. We will make use of this class for most of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b20b4ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:06<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset of image patches.\n",
    "cd.create_patches(discovered_concepts_dir, param_dict={'n_segments': [15]})\n",
    "\n",
    "# Saving the concept discovery target class images.\n",
    "image_dir = discovered_concepts_dir / 'images'\n",
    "image_dir.mkdir()\n",
    "ace_helpers.save_images(image_dir.absolute(),\n",
    "                        (cd.discovery_images * 256).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feedd4b",
   "metadata": {},
   "source": [
    "We can now check the output directory to find the raw discovery images, the superpixels and the patches. These can now be used to find potential visual concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e5c2cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating activations for superpixels: 100%|█████████████████████████████████████████| 31/31 [00:43<00:00,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# Discovering Concepts\n",
    "cd.discover_concepts(discovered_concepts_dir, method='KM', param_dicts={'n_clusters': 25})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317e788a",
   "metadata": {},
   "source": [
    "After running the above code we have collected the activations from passing the superpixels through our model. After optionally getting the average across filters to reduce size and then flattening, we are ready to cluster. This is also carried out above, leaving us with a folder of images for each of these potential concepts. These can be seen in our output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad517a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save discovered concept images (resized and original sized)\n",
    "ace_helpers.save_concepts(cd, discovered_concepts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1f74e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add to helper function generate_random\n",
    "superpixels = discovered_concepts_dir / \"superpixels\"\n",
    "list_of_files = list(superpixels.iterdir())\n",
    "\n",
    "# Random selection of the the superpixels for random concept?\n",
    "random.seed(42)\n",
    "cd.random_imgs = np.array(random.sample(list_of_files, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfd7a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the random imgs for review save_random\n",
    "for img in cd.random_imgs:\n",
    "    destination = img.parent.parent / \"Random\"\n",
    "    destination.mkdir(exist_ok=True)\n",
    "    \n",
    "    shutil.copy(img, destination / img.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fe6ee2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating activations for superpixels: 100%|█████████████████████████████████████████| 25/25 [00:22<00:00,  1.12it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 5/5 [00:04<00:00,  1.18it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 5/5 [00:04<00:00,  1.14it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 5/5 [00:04<00:00,  1.19it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 4/4 [00:03<00:00,  1.12it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 5/5 [00:03<00:00,  1.25it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 5/5 [00:04<00:00,  1.12it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 4/4 [00:03<00:00,  1.30it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 5/5 [00:04<00:00,  1.16it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 5/5 [00:04<00:00,  1.20it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 5/5 [00:04<00:00,  1.20it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 4/4 [00:02<00:00,  1.37it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 5/5 [00:04<00:00,  1.21it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 5/5 [00:04<00:00,  1.21it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 3/3 [00:02<00:00,  1.19it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 5/5 [00:04<00:00,  1.20it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 3/3 [00:02<00:00,  1.20it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 5/5 [00:04<00:00,  1.20it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 5/5 [00:04<00:00,  1.20it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 5/5 [00:04<00:00,  1.20it/s]\n",
      "Calculating activations for superpixels: 100%|███████████████████████████████████████████| 5/5 [00:04<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "cav_accuracies = cd.cavs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f09d1291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backbone.body.layer1.2.conv1': {'tennis racket_concept1': 0.5,\n",
       "  'tennis racket_concept2': 0.5,\n",
       "  'tennis racket_concept3': 0.75,\n",
       "  'tennis racket_concept4': 0.5},\n",
       " 'backbone.body.layer2.3.conv1': {'tennis racket_concept1': 0.5,\n",
       "  'tennis racket_concept2': 0.5,\n",
       "  'tennis racket_concept3': 0.6666666666666666,\n",
       "  'tennis racket_concept4': 0.5,\n",
       "  'tennis racket_concept5': 0.5},\n",
       " 'backbone.body.layer3.5.conv1': {'tennis racket_concept1': 0.5,\n",
       "  'tennis racket_concept2': 0.6666666666666666,\n",
       "  'tennis racket_concept3': 0.5,\n",
       "  'tennis racket_concept4': 0.75,\n",
       "  'tennis racket_concept5': 0.6666666666666666,\n",
       "  'tennis racket_concept6': 0.5,\n",
       "  'tennis racket_concept7': 0.0},\n",
       " 'backbone.body.layer4.2.conv1': {'tennis racket_concept1': 0.75,\n",
       "  'tennis racket_concept2': 0.75,\n",
       "  'tennis racket_concept3': 0.75,\n",
       "  'tennis racket_concept4': 0.5}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cav_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0281909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating gradients:   0%|                                                                    | 0/86 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 304])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 152])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 76])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 38])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 304])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 152])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 76])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 38])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 304])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 152])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 76])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 38])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 304])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 152])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 76])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 38])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating gradients:   1%|▋                                                           | 1/86 [00:03<04:42,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 304])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 152])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 76])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 38])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 304, 200])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 152, 100])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 76, 50])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 38, 25])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 304, 200])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 152, 100])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 76, 50])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 38, 25])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 304, 200])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 152, 100])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 76, 50])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 38, 25])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 304, 200])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 152, 100])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 76, 50])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 38, 25])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 304, 200])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 152, 100])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 76, 50])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 38, 25])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 304, 200])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 152, 100])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 76, 50])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 38, 25])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 304, 200])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 152, 100])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 76, 50])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 38, 25])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating gradients:   2%|█▍                                                          | 2/86 [00:08<05:49,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 304, 200])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 152, 100])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 76, 50])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 38, 25])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating gradients:   3%|██                                                          | 3/86 [00:10<04:53,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 288])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 144])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 72])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 36])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 288])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 144])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 72])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 36])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating gradients:   5%|██▊                                                         | 4/86 [00:12<03:59,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 288])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 144])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 72])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 36])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 288])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 144])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 72])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 36])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 288])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 144])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 72])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 36])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 288])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 144])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 72])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 36])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 288])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 144])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 72])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 36])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 288])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 144])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 72])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 36])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 288])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 144])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 72])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 36])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 288])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 144])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 72])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 36])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating gradients:   6%|███▍                                                        | 5/86 [00:17<04:49,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 288])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 144])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 72])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 36])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 296])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 148])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 74])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 37])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 296])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 148])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 74])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 37])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating gradients:   7%|████▏                                                       | 6/86 [00:19<04:09,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 296])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 148])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 74])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 37])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating gradients:   8%|████▉                                                       | 7/86 [00:21<03:25,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating gradients:   9%|█████▌                                                      | 8/86 [00:25<04:00,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating gradients:  10%|██████▎                                                     | 9/86 [00:35<06:39,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 272])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 136])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 68])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 34])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 248])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 124])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 62])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 31])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Calculating gradients:  12%|██████▊                                                    | 10/86 [00:36<05:10,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 200, 248])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 100, 124])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 50, 62])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 25, 31])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 304, 200])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 152, 100])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 76, 50])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 38, 25])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 304, 200])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 152, 100])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 76, 50])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 38, 25])\n",
      "backbone.body.layer1.2.conv1 torch.Size([1, 64, 304, 200])\n",
      "backbone.body.layer2.3.conv1 torch.Size([1, 128, 152, 100])\n",
      "backbone.body.layer3.5.conv1 torch.Size([1, 256, 76, 50])\n",
      "backbone.body.layer4.2.conv1 torch.Size([1, 512, 38, 25])\n"
     ]
    }
   ],
   "source": [
    "scores = cd.tcavs(test=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5746ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating CAVs and TCAV scores\n",
    "cav_accuracies = cd.cavs(min_acc=0.0)\n",
    "scores = cd.tcavs(test=False)\n",
    "ace_helpers.save_ace_report(cd, cav_accuracies, scores,\n",
    "                            results_summaries_dir + 'ace_results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c7a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot examples of discovered concepts\n",
    "for bn in cd.bottlenecks:\n",
    "    ace_helpers.plot_concepts(cd, bn, 10, address=results_dir)\n",
    "# Delete concepts that don't pass statistical testing\n",
    "cd.test_and_remove_concepts(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.model.model.model.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c117c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = cd.source_dir / cd.target_class\n",
    "tcav_score_images = list(files.iterdir())[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [T.ToTensor()(Image.open(tcav_score_images[0]))]\n",
    "tensor_imgs = torch.stack(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebcb5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "old = np.array([np.array(Image.open(tcav_score_images[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de3f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tensor_imgs = torch.from_numpy(old).float()\n",
    "\n",
    "if len(old_tensor_imgs.shape) < 4:    \n",
    "    old_tensor_imgs = old_tensor_imgs[None, :]\n",
    "\n",
    "\n",
    "old_tensor_imgs = old_tensor_imgs.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the GPU if it is available.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ba3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_imgs = tensor_imgs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7421098f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd.model.model(tensor_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa8799",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = cd.model.model.generate_gradients(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb27b5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tensor_imgs = old_tensor_imgs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699347e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd.model.model.intermediate_activations[\"backbone.body.layer1.2.conv1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f6fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = cd.model.model.generate_gradients(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7bd1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grads, info = cd._return_gradients(tcav_score_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for grad in grads[\"backbone.body.layer1.2.conv1\"]:\n",
    "    print(grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac65f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cd._tcav_score(\"backbone.body.layer1.2.conv1\", \"tennis racket_concept1\", \"Random\", grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a1e109",
   "metadata": {},
   "source": [
    "Mitotic figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e030fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output directory for our data\n",
    "output = Path.cwd() / \"ACE_mitotic_output/\"\n",
    "\n",
    "# Create the relevant sub-directories.\n",
    "discovered_concepts_dir = output / 'concepts/'\n",
    "results_dir = output / 'results/'\n",
    "cavs_dir = output / 'cavs/'\n",
    "activations_dir = output / 'acts/'\n",
    "results_summaries_dir = output / 'results_summaries/'\n",
    "\n",
    "# # If the directory exists we delete it to generate new output.\n",
    "# if output.exists():\n",
    "#     shutil.rmtree(output)\n",
    "\n",
    "# # Make all of the directories\n",
    "# output.mkdir()\n",
    "# discovered_concepts_dir.mkdir()\n",
    "# results_dir.mkdir()\n",
    "# cavs_dir.mkdir()\n",
    "# activations_dir.mkdir()\n",
    "# results_summaries_dir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72907271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the target class and the source directory.\n",
    "target_class = \"mitotic figure\"\n",
    "source_dir = \"D:\\DS\\DS4\\Project\\MIDOG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf4eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Random concept for statistical testing.\n",
    "random_concept = 'random_discovery'\n",
    "\n",
    "# Define the bottleneck layers.\n",
    "\n",
    "# Create the model variable and set it to evaluate.\n",
    "mymodel = ace_helpers.MyModel(\"mitotic\", ['backbone.body.layer1.2.conv1', 'backbone.body.layer2.3.conv1', 'backbone.body.layer3.5.conv1', 'backbone.body.layer4.2.conv1'])\n",
    "mymodel.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the ConceptDiscovery class instance.\n",
    "cd = ConceptDiscovery(\n",
    "    mymodel,\n",
    "    target_class,\n",
    "    random_concept,\n",
    "    ['backbone.body.layer1.2.conv1', 'backbone.body.layer2.3.conv1', 'backbone.body.layer3.5.conv1', 'backbone.body.layer4.2.conv1'],\n",
    "    source_dir,\n",
    "    activations_dir,\n",
    "    cavs_dir,\n",
    "    num_random_exp=2,\n",
    "    channel_mean=True,\n",
    "    max_imgs=100,\n",
    "    min_imgs=50,\n",
    "    num_discovery_imgs=100,\n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset of image patches.\n",
    "cd.create_patches(discovered_concepts_dir, param_dict={'n_segments': [15]})\n",
    "\n",
    "# Saving the concept discovery target class images.\n",
    "image_dir = discovered_concepts_dir / 'images'\n",
    "image_dir.mkdir()\n",
    "ace_helpers.save_images(image_dir.absolute(),\n",
    "                        (cd.discovery_images * 256).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ac0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discovering Concepts\n",
    "cd.discover_concepts(discovered_concepts_dir, method='KM', param_dicts={'n_clusters': 25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f433d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save discovered concept images (resized and original sized)\n",
    "ace_helpers.save_concepts(cd, discovered_concepts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47802632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add to helper function generate_random\n",
    "superpixels = discovered_concepts_dir / \"superpixels\"\n",
    "list_of_files = list(superpixels.iterdir())\n",
    "\n",
    "# Random selection of the the superpixels for random concept?\n",
    "random.seed(42)\n",
    "cd.random_imgs = np.array(random.sample(list_of_files, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc936f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the random imgs for review save_random\n",
    "for img in cd.random_imgs:\n",
    "    destination = img.parent.parent / \"Random\"\n",
    "    destination.mkdir(exist_ok=True)\n",
    "    \n",
    "    shutil.copy(img, destination / img.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e1ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cav_accuracies = cd.cavs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e68ed60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
