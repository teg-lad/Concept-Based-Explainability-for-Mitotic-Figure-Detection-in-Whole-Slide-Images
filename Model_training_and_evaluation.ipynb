{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2248f98",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "This notebook contains all of the code used to train the model used in my project. This model makes use of the PyTorch library and the pre-trained Faster R-CNN model provided in the TorchVision library. The structure of the model can be changed to accomadate the different output classes and then finetuned on my training data. The documentation for the Faster R-CNN implementation can be found [here](https://pytorch.org/vision/stable/models/faster_rcnn.html).\n",
    "\n",
    "The primary components of this model are a region proposal network (RPN), a Fast-RCNN classifier and a backbone. The pytorch stable 1.13.1 and cuda 11.7 versions were used in this environment.\n",
    "\n",
    "### Imports and Setup\n",
    "\n",
    "The following imports are needed to run the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067def1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as f\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import Utils.Training.utils as utils\n",
    "from Utils.Training.training import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f004543b",
   "metadata": {},
   "source": [
    "Before we start, we should check that cuda is installed properly and we can use any GPU we have on our machine with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d209dc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "GPU name: NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available.\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "print(f\"GPU available: {cuda_available}\")\n",
    "\n",
    "try:\n",
    "    # Get the cuda device name if available.\n",
    "    name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU name: {name}\")\n",
    "    \n",
    "except AssertionError:\n",
    "    print(\"GPU not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb371f",
   "metadata": {},
   "source": [
    "### Model inspection\n",
    "\n",
    "Firstly, I will load the default model, inspect the architecture and ensure that it works as expected. This can be done by using the model to make a prediction on an image from the COCO dataset (which it was trained on). In this way I can better understand how the model should be used and ensure it has the pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b074bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Load the model weights into the model and save as an object. Set the model to evaluation mode so we can inference.\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de58d363",
   "metadata": {},
   "source": [
    "We have loaded the weights into the model object and set the model to evaluation mode. Now we can use print on the model to output all of the information regarding the layers and different connections. We can see several different componenets.\n",
    "\n",
    "+ Transform - the transformations to be applied to an image before it can be passed through the model.\n",
    "+ backbone (with FPN) - This part of the model is responsible for feature extraction and consists of sequential bottlenecks to create compressed feature representations. The feature pyramid network (FPN) allows for context at different levels by outputing feature maps at multiple levels.\n",
    "+ Region proposal network (RPN) - Network for proposing regions with possible object of interest that are used during the final stage of detection.\n",
    "+ Fast R-CNN predictor - Takes the regions and the features maps and returns the detections.\n",
    "\n",
    "This is the basic overview of our model that can be seen in the below output. This can be finetuned to our specific task with some minor alterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d28db49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform(\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
      "  )\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (extra_blocks): LastLevelMaxPool()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2dNormActivation(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Flatten(start_dim=1, end_dim=-1)\n",
      "      (5): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (6): ReLU(inplace=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13f25c",
   "metadata": {},
   "source": [
    "Now we can take a sample image from the COCO dataset, which the default weights for the Faster R-CNN model were obtained from. Using this we can see how the model works and also how well it works.\n",
    "\n",
    "This will involve reading the image, converting it to an appropriate format and passing it to the model to return predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1d043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a COCO image and convert it to RGB, it was read as RGBA originally.\n",
    "img = Image.open(\"D:/DS/DS4/Project/COCO/tennis racket/COCO_test2014_000000000057.jpg\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the image.\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49229597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to a tensor and add an empty dimension so we have [number images x channels x height x width].\n",
    "tensor_img = T.ToTensor()(img)\n",
    "tensor_img = tensor_img[None, :]\n",
    "print(tensor_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass our example image through the model to get the predictions.\n",
    "# Note: We don't need the GPU here as there is only one image to inference on.\n",
    "predictions = model(tensor_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def74ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the format of the returned predictions.\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924c537",
   "metadata": {},
   "source": [
    "We can see that the predictions are a list of dictionaries for boxes, labels and scores. All of these are of equal length with the element in position 0 in all three being related. So we can take the list of boxes and labels and they will correspond.\n",
    "\n",
    "We can look up the COCO dataset labels to find out the mapping from class index to class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae565e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the boxes out of the prediction.\n",
    "boxes = predictions[0][\"boxes\"]\n",
    "\n",
    "# These labels were manually entered from the COCO dataset class list.\n",
    "labels = [\"person\", \"sports ball\", \"tennis racket\", \"clock\", \"sports ball\"]\n",
    "\n",
    "# Take just the first image, which just involves removing the first dimension which corresponds to the number of images.\n",
    "# which is 1 in our case. Squeeze removes dimensions of length 1.\n",
    "example = torch.squeeze(tensor_img)\n",
    "\n",
    "# Convert it to int8. \n",
    "example = T.ConvertImageDtype(torch.uint8)(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57eb056",
   "metadata": {},
   "source": [
    "We can make use of the draw_bounding_boxes method to display the boxes and labels on a given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2cf3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use draw_bounding_boxes to overlay the bounding boxes and labels on to the image.\n",
    "img_with_boxes = draw_bounding_boxes(example, boxes, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958fe58c",
   "metadata": {},
   "source": [
    "Now we can take this image and display it using pyplot.imshow, but we first have to re-order the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a85017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to change the ordering as the channels should be the last dimensions for pyplot.\n",
    "plt.imshow(img_with_boxes.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116ad45",
   "metadata": {},
   "source": [
    "We can see that the model does a good job at finding the object that are present in the COCO dataset classes, meaning the pre-trained weights are good. This is still a good sanity check that the model weights are meaningful and will hopefully be a good starting point for the creation of my model.\n",
    "\n",
    "### Dataset and Dataloader\n",
    "\n",
    "Now that we have inspected the model and have a better understanding of the architecture, how to use it and the input and output types we are ready to start fine-tuning it. The first step is to get our data loaded in by creating a PyTorch Dataset and then making use of a Dataloader to load the data in in batches.\n",
    "\n",
    "The Dataset class below implements the relevant functions to read our Midog data in as a Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd0b774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset class inheriting from the Dataset\n",
    "class MidogDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class for implementing a dataset for the data for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root, transforms=None):\n",
    "        \"\"\"\n",
    "        root - Path to the directory storing the data\n",
    "        transforms - transform method to be applied to the data\n",
    "        \"\"\"\n",
    "        \n",
    "        # Store the root and transforms as class variables.\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # load all image files, sorting them to ensure that they are aligned.\n",
    "        # Save the images into an instance variable.\n",
    "        self.imgs = list(sorted(os.listdir(Path(root, \"data\"))))\n",
    "        \n",
    "        # Open the training json and load in the data.\n",
    "        with open(os.path.join(root, \"training.json\")) as t:   \n",
    "            training_data = json.load(t)\n",
    "        \n",
    "        # Store the data into an instance variable.\n",
    "        self.data = training_data[\"images\"]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the data for a given index\n",
    "        \n",
    "        idx - The index of the data item to be returned\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the image path and load the image into memory.\n",
    "        img_path = Path(self.root, \"data\", self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Parse the image path to get the image and tild id.\n",
    "        image_id, tile_id = self.imgs[idx].split(\".\")[0].split(\"_\")\n",
    "\n",
    "        # Get the tile info for the given image and tile id.\n",
    "        tile_info = self.get_tile_info(image_id, tile_id)\n",
    "\n",
    "        # Extract the bounding boxes from the tile annotations.\n",
    "        boxes = []\n",
    "        for anno in tile_info[\"annotations\"]:\n",
    "            left, bottom, right, top = anno[\"bounding_box\"].values()\n",
    "            \n",
    "            boxes.append([left, bottom, right, top])\n",
    "        \n",
    "        # Get the number of objects so we can create the labels.\n",
    "        num_objs = len(boxes)\n",
    "        \n",
    "        # There is only one class, mitotic figure, so all labels are 1.\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Convert the boxes to a tensor of int32.\n",
    "        # Note: int 16 is enough to hold the information we have but the model expects the targets in int32 format.\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.int32)\n",
    "\n",
    "        # Create the target dictionary to be returned for a data item.\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"tile_id\"] = tile_id\n",
    "    \n",
    "\n",
    "        # If there are any transforms defined we can carry them out here.\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        # The number of data items we have the length of our list of image paths.\n",
    "        # We could have used the individual annotations as a unit of data but this adds additional complexity and \n",
    "        # model training is not the primary objective of this project.\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def get_tile_info(self, image_id, tile_id):\n",
    "        \"\"\"\n",
    "        Return the dictionary that contains information relating to the given image and tile id.\n",
    "        \n",
    "        image_id - The int id of the image we want the data for.\n",
    "        tile_id - The int id of the tile we want the data for.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the dictionary with information on the given image.\n",
    "        image_info = next((image for image in self.data if image[\"image_id\"] == int(image_id)), None)\n",
    "        \n",
    "        # Within this image dictionary, get the dictionary that relates to the given tile id.\n",
    "        tile_info = next((tile for tile in image_info[\"tiles\"] if tile[\"tile_id\"] == int(tile_id)), None)\n",
    "        \n",
    "        return tile_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a40982eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transformation for the dataset so can have the image as a tensor for use with the model.\n",
    "def transforms(img, target):\n",
    "    return T.ToTensor()(img), target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc6fbb0",
   "metadata": {},
   "source": [
    "With our dataset defined, we can now create a dataset object by giving the path to the directory containing the training images and json. Then we can use a DataLoader to iterate through the data and return batches of a specified size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c9f8cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a Dataset object with the given root path to the training data and a defined transformation.\n",
    "midog = MidogDataset(\"D:/DS/DS4/Project/Training_mitotic_figures\", transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader with the dataset with a batch size of 2, no shuffling and use a custom defined collate_fn to batch\n",
    "# the output as desired.\n",
    "data_loader = torch.utils.data.DataLoader(midog, batch_size=2, shuffle=False, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator from the DataLoader and take the next item from the iterator.\n",
    "images, target = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3efe8",
   "metadata": {},
   "source": [
    "Using this batch of images from the dataset we can compute predictions. The classes returned will be meaningless as the COCO dataset that the model was originally trained on does not contain mitotic figure as a class. It will be interesting to see if the model can detect anything of interest in these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b044a2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the images to the model, which is in evaluation mode already.\n",
    "predictions = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de374a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the boxes out of the prediction.\n",
    "boxes = predictions[0][\"boxes\"]\n",
    "\n",
    "# Concatenate the prediction boxes and the ground truth boxes so we can view both.\n",
    "final_boxes = torch.cat([boxes, target[0][\"boxes\"]])\n",
    "\n",
    "# Create a list of labels using the length of the predictions and ground truths.\n",
    "labels = ([\"pred\"] * len(boxes)) + ([\"gt\"] * len(target[0][\"boxes\"]))\n",
    "\n",
    "# Take just the first image.\n",
    "# Note: We can't use squeeze as the first dimension is not 1 this time, but 2 as our batch size is 2.\n",
    "example = images[0,:]\n",
    "\n",
    "# Convert the image to int8.\n",
    "example = T.ConvertImageDtype(torch.uint8)(example) # Convert it to int8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c24844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use draw_bounding_boxes to overlay the bounding boxes and labels on to the image.\n",
    "img_with_boxes = draw_bounding_boxes(example, final_boxes, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to change the ordering as the channels should be the last dimensions for pyplot.\n",
    "plt.imshow(img_with_boxes.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e2af1",
   "metadata": {},
   "source": [
    "Unfortunately, there does not seem to be any meaningful predictions being made by the model with default weights. This is to be expected as the task is in a different domain. Once we have trained the model on our data we will see much more reasonable predictions.\n",
    "\n",
    "### Model Alteration\n",
    "\n",
    "Now we need to make some alterations to our model before we train it. We need to change the output number of classes, so we need to create a new predictor that takes the feature maps and predicts the class and bounding box for an object. This can be done as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5bb1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (mitotic figure) + background\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae96a3a",
   "metadata": {},
   "source": [
    "Now our model is ready to be fine-tuned.\n",
    "\n",
    "### Data sanity check\n",
    "\n",
    "Before training I will do a quick sanity check of the data to ensure it is all as it should be.\n",
    "\n",
    "It turns out that there was a bug in my bounding box generation process that didn't account for tile difference when a bounding box was negative i.e. on the padding to the left or under the original tile. This has been fixed and now my sanity checks are as expected. Helping to preserve my sanity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8208c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use our dataset and defined transformations.\n",
    "dataset = MidogDataset(\"D:/DS/DS4/Project/Training_mitotic_figures\", transforms)\n",
    "\n",
    "# Initiate the DataLoader.\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=False, collate_fn=utils.collate_fn)\n",
    "\n",
    "# For every image and target in the iterator\n",
    "for images, targets in data_loader:\n",
    "    try:\n",
    "        # Check that no values are outside of the boudanries (412 x 412 original and 50 pixel padding all around, so 512 x 512)\n",
    "        assert(max([t[\"boxes\"].numpy().max() for t in targets]) <= 512)\n",
    "        assert(min([t[\"boxes\"].numpy().min() for t in targets]) >= 0)\n",
    "\n",
    "        # Stack all of the boxes vertically.\n",
    "        boxes = np.vstack([t[\"boxes\"].numpy() for t in targets])\n",
    "\n",
    "        # Check that the x1 < x2 for all of these boxes.\n",
    "        assert(np.all(boxes[:,0] < boxes[:,2]) == True)\n",
    "\n",
    "        # Check that y1 < y2 for all of these boxes.\n",
    "        assert(np.all(boxes[:,1] < boxes[:,3]) == True)\n",
    "    \n",
    "    # If we encounter an AssertionError, print the dictionary of annotations that caused it.\n",
    "    except AssertionError:\n",
    "        print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb526b",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "The process of training our model consists of multiple steps.\n",
    "\n",
    "Firstly we create our dataset and split it into a train and test set. We can make use of torch.manual_seed to ensure that the random premutation we get for the indices stays the same on every run. There are 7549 training images, some of these may have more than 1 example on them. Let us take ~10% of the data for testing, which is 750 tiles for testing (375 batches of 2 images).\n",
    "\n",
    "Now that we have our data, we need to consider the loss. This is returned in the form of a classification and regression loss for the RPN and the R-CNN. These can be summed to get the total loss which can be used to compute the optimization steps.\n",
    "\n",
    "We will make use of the Adam optimizer, which makes use of both Momentum and root mean square propagation to converge faster.\n",
    "\n",
    "This covers our main training loop that updates our weights. After every epoch we would like to run the model on our test set and look at some metrics. We will make use of TorchMetrics to compute the mAP (MeanAveragePrecision) for the test set. In addition, looking at the test loss will allow us to know when we should stop training. In this way, we can avoid overfitting the training data.\n",
    "\n",
    "TorchMetrics is easy to use and involves passing the ground truths and predictions to the object created from the MeanAveragePrecision class. Returning the test loss is a different story. The classes for the components of the model in Torchvision don't return the loss when in evaluation. To remedy this, I altered the logic in these files to ensure that if the ground truths are provided the loss is returned. This preseves the previous functionality while allowing me to return the loss.\n",
    "\n",
    "The below code is the original final output from the GeneralizedRCNN class:\n",
    "\n",
    "``` \n",
    "def eager_outputs(self, losses, detections):\n",
    "        # type: (Dict[str, Tensor], List[Dict[str, Tensor]]) -> Union[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
    "        if self.training:\n",
    "            return losses\n",
    "\n",
    "        return detections\n",
    "```\n",
    "\n",
    "This is what I changed it to:\n",
    "``` \n",
    "def eager_outputs(self, losses, detections):\n",
    "        # type: (Dict[str, Tensor], List[Dict[str, Tensor]]) -> Union[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
    "        if self.training:\n",
    "            return losses\n",
    "\n",
    "        elif losses:\n",
    "            return losses, detections\n",
    "\n",
    "        return detections\n",
    "``` \n",
    "\n",
    "Changes also had to be made to the logic in the components of the GeneralizedRCNN, namely the classes in roi_heads.py and rpn.py. These altered files will be included in the repositiory so the test loss can be computed during the following training.\n",
    "\n",
    "With all of this, we are ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86c81636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorBoard SummaryWriter so we can write metrics to the TensorBoard logs for monitoring.\n",
    "# Note: This was not useful as sessions don't seem to work well when the SummaryWriter is initialized again, when training at\n",
    "# another point in time.\n",
    "writer = SummaryWriter('D:/DS/DS4/Project/tensorboard_logs/midog_training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4efa7c",
   "metadata": {},
   "source": [
    "Now we can load our dataset and pass it to our train method. This method and other methods that are called within train can be found under Training in the Utils folder of the projecy repo.\n",
    "\n",
    "The model can be trained from scratch, or a checkpoint file from torch.save can be passed that contains the model state_dict, the optimizer state_dict and the current epoch.\n",
    "\n",
    "Using all of the data takes roughly an hour and 30 minutes to train one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb4cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MidogDataset(\"D:/DS/DS4/Project/Training_mitotic_figures\", transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a9687cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [0/5]  eta: 0:00:18  lr: 0.001254  loss: 0.8549 (0.8549)  loss_classifier: 0.6781 (0.6781)  loss_box_reg: 0.0016 (0.0016)  loss_objectness: 0.1720 (0.1720)  loss_rpn_box_reg: 0.0032 (0.0032)  time: 3.7515  data: 0.0312  max mem: 2771\n",
      "Epoch: [0]  [4/5]  eta: 0:00:01  lr: 0.005000  loss: 0.6202 (0.6318)  loss_classifier: 0.4149 (0.4867)  loss_box_reg: 0.0346 (0.0387)  loss_objectness: 0.0847 (0.0936)  loss_rpn_box_reg: 0.0027 (0.0128)  time: 1.8986  data: 0.0281  max mem: 3215\n",
      "Epoch: [0] Total time: 0:00:09 (1.8988 s / it)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1]  [0/5]  eta: 0:00:09  lr: 0.005000  loss: 3.3869 (3.3869)  loss_classifier: 2.4295 (2.4295)  loss_box_reg: 0.6247 (0.6247)  loss_objectness: 0.2936 (0.2936)  loss_rpn_box_reg: 0.0391 (0.0391)  time: 1.8806  data: 0.0311  max mem: 3220\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initial training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDS\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDS4\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProject\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmodel_saves\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\DS\\DS4\\Project\\2023-ca4021-tegarta2\\Utils\\Training\\training.py:229\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataset, num_epochs, output_dir, writer, checkpoint)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# For each epoch from the current until we have carried out as many as specified.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch, epoch \u001b[38;5;241m+\u001b[39m num_epochs):\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# Train for one epoch, printing every 100 iterations.\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m     training_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;66;03m# Update the learning rate.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m     lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mD:\\DS\\DS4\\Project\\2023-ca4021-tegarta2\\Utils\\Training\\training.py:103\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch, print_freq, writer, scaler)\u001b[0m\n\u001b[0;32m    100\u001b[0m         lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;66;03m# Update the metric logger\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     \u001b[43mmetric_logger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlosses_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_dict_reduced\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     metric_logger\u001b[38;5;241m.\u001b[39mupdate(lr\u001b[38;5;241m=\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Compute the average loss per batch\u001b[39;00m\n",
      "File \u001b[1;32mD:\\DS\\DS4\\Project\\2023-ca4021-tegarta2\\Utils\\Training\\utils.py:121\u001b[0m, in \u001b[0;36mMetricLogger.update\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 121\u001b[0m         v \u001b[38;5;241m=\u001b[39m \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m))\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeters[k]\u001b[38;5;241m.\u001b[39mupdate(v)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training from scratch, no checkpoint is given.\n",
    "train(model, dataset, 10, \"D:\\DS\\DS4\\Project\\model_saves\", writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e1084",
   "metadata": {},
   "source": [
    "If we would like to continue training from a checkpoint we can use the below code and read in the checkpoint and pass it to the train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef173ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training from a checkpoint, which is passed as the last parameter to train.\n",
    "checkpoint = torch.load(\"D:/DS/DS4/Project/model_saves/2023_03_02_16_35_46_8.pth\", map_location=lambda storage, loc: storage)\n",
    "train(model, dataset, 10, \"D:\\DS\\DS4\\Project\\model_saves\", writer, checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a2a948",
   "metadata": {},
   "source": [
    "### Training overview\n",
    "\n",
    "Once we have trained our model, we can look at the training and test loss to determine the best model for us to use.\n",
    "\n",
    "Following this, we can inspect the metrics and see how well this model performs using the standard COCO challenge metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214fbf2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87a41426",
   "metadata": {},
   "source": [
    "### Review trained model predictions\n",
    "\n",
    "We can now load the model and see how well the predictions look visually for images from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the best model by specifying the path to the save.\n",
    "checkpoint = torch.load(\"D:/DS/DS4/Project/model_saves/2023_03_04_17_22_33_11.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f23696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the model that we have already initialized and load the state_dict into it.\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "# Set it to evaluation mode.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b23b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our dataset and defined transformations.\n",
    "dataset = MidogDataset(\"D:/DS/DS4/Project/Training_mitotic_figures\", transforms)\n",
    "\n",
    "# Split the dataset into the train and test set, using the same random seed to ensure the same split.\n",
    "torch.manual_seed(42)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset_test = torch.utils.data.Subset(dataset, indices[-750:])\n",
    "\n",
    "# Create the DataLoader for the test dataset.\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "# Create an iterator from the DataLoader.\n",
    "iter_data = iter(data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d872a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the next batch from the DataLoader\n",
    "images, targets = next(iter_data)\n",
    "\n",
    "# Use the GPU if it is available.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Move both the model and the data to the device, ideally the GPU.\n",
    "model.to(device)\n",
    "images = images.to(device)\n",
    "targets = [{k: v.to(device) for k, v in t.items() if k in [\"boxes\", \"labels\"]} for t in targets]\n",
    "\n",
    "# Don't calculate gradients as we aren't updating the weights and this will save memory.\n",
    "with torch.no_grad():\n",
    "    predictions = model(images)\n",
    "\n",
    " # Pull the boxes out of the prediction.\n",
    "boxes = predictions[0][\"boxes\"]\n",
    "\n",
    "# Concatenate the prediction and ground truth bounding boxes.\n",
    "final_boxes = torch.cat([boxes, targets[0][\"boxes\"]])\n",
    "\n",
    "# Create a list of labels using the length of the list of predictions and list of ground truths.\n",
    "labels = ([\"pred\"] * len(boxes)) + ([\"gt\"] * len(targets[0][\"boxes\"]))\n",
    "\n",
    "# Reduce the dimension to just channel x height x width as we have only one image in the batch and want to display it.\n",
    "example = torch.squeeze(images)\n",
    "\n",
    "# Convert the image to int8\n",
    "example = T.ConvertImageDtype(torch.uint8)(example)\n",
    "\n",
    "\n",
    "img_with_boxes = draw_bounding_boxes(example, final_boxes, labels)\n",
    "\n",
    "# We need to change the ordering as the channels should be the last dimensions for pyplot.\n",
    "plt.imshow(img_with_boxes.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04499fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03554cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
