{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2248f98",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "This notebook contains all of the code used to train the model used in my project. This model makes use of the PyTorch library and the pre-trained Faster R-CNN model provided in the TorchVision library. The structure of the model can be changed to accomadate the different output classes and then finetuned on my training data. The documentation for the Faster R-CNN implementation can be found [here](https://pytorch.org/vision/stable/models/faster_rcnn.html).\n",
    "\n",
    "The primary components of this model are a region proposal network (RPN), a Fast-RCNN classifier and a backbone. The pytorch stable 1.13.1 and cuda 11.7 versions were used in this environment.\n",
    "\n",
    "### Imports and Setup\n",
    "\n",
    "The following imports are needed to run the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "067def1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import ast\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import Utils.Training.utils as utils\n",
    "from Utils.Training.training import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f004543b",
   "metadata": {},
   "source": [
    "Before we start, we should check that cuda is installed properly and we can use any GPU we have on our machine with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d209dc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "GPU name: NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available.\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "print(f\"GPU available: {cuda_available}\")\n",
    "\n",
    "try:\n",
    "    # Get the cuda device name if available.\n",
    "    name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU name: {name}\")\n",
    "    \n",
    "except AssertionError:\n",
    "    print(\"GPU not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb371f",
   "metadata": {},
   "source": [
    "### Model inspection\n",
    "\n",
    "Firstly, I will load the default model, inspect the architecture and ensure that it works as expected. This can be done by using the model to make a prediction on an image from the COCO dataset (which it was trained on). In this way I can better understand how the model should be used and ensure it has the pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Load the model weights into the model and save as an object. Set the model to evaluation mode so we can inference.\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de58d363",
   "metadata": {},
   "source": [
    "We have loaded the weights into the model object and set the model to evaluation mode. Now we can use print on the model to output all of the information regarding the layers and different connections. We can see several different componenets.\n",
    "\n",
    "+ Transform - the transformations to be applied to an image before it can be passed through the model.\n",
    "+ backbone (with FPN) - This part of the model is responsible for feature extraction and consists of sequential bottlenecks to create compressed feature representations. The feature pyramid network (FPN) allows for context at different levels by outputing feature maps at multiple levels.\n",
    "+ Region proposal network (RPN) - Network for proposing regions with possible object of interest that are used during the final stage of detection.\n",
    "+ Fast R-CNN predictor - Takes the regions and the features maps and returns the detections.\n",
    "\n",
    "This is the basic overview of our model that can be seen in the below output. This can be finetuned to our specific task with some minor alterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d28db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13f25c",
   "metadata": {},
   "source": [
    "Now we can take a sample image from the COCO dataset, which the default weights for the Faster R-CNN model were obtained from. Using this we can see how the model works and also how well it works.\n",
    "\n",
    "This will involve reading the image, converting it to an appropriate format and passing it to the model to return predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1d043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a COCO image and convert it to RGB, it was read as RGBA originally.\n",
    "img = Image.open(\"D:/DS/DS4/Project/COCO/tennis racket/COCO_test2014_000000000057.jpg\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the image.\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49229597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to a tensor and add an empty dimension so we have [number images x channels x height x width].\n",
    "tensor_img = T.ToTensor()(img)\n",
    "tensor_img = tensor_img[None, :]\n",
    "print(tensor_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass our example image through the model to get the predictions.\n",
    "# Note: We don't need the GPU here as there is only one image to inference on.\n",
    "predictions = model(tensor_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def74ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the format of the returned predictions.\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924c537",
   "metadata": {},
   "source": [
    "We can see that the predictions are a list of dictionaries for boxes, labels and scores. All of these are of equal length with the element in position 0 in all three being related. So we can take the list of boxes and labels and they will correspond.\n",
    "\n",
    "We can look up the COCO dataset labels to find out the mapping from class index to class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae565e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the boxes out of the prediction.\n",
    "boxes = predictions[0][\"boxes\"]\n",
    "\n",
    "# These labels were manually entered from the COCO dataset class list.\n",
    "labels = [\"person\", \"sports ball\", \"tennis racket\", \"clock\", \"sports ball\"]\n",
    "\n",
    "# Take just the first image, which just involves removing the first dimension which corresponds to the number of images.\n",
    "# which is 1 in our case. Squeeze removes dimensions of length 1.\n",
    "example = torch.squeeze(tensor_img)\n",
    "\n",
    "# Convert it to int8. \n",
    "example = T.ConvertImageDtype(torch.uint8)(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57eb056",
   "metadata": {},
   "source": [
    "We can make use of the draw_bounding_boxes method to display the boxes and labels on a given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2cf3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use draw_bounding_boxes to overlay the bounding boxes and labels on to the image.\n",
    "img_with_boxes = draw_bounding_boxes(example, boxes, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958fe58c",
   "metadata": {},
   "source": [
    "Now we can take this image and display it using pyplot.imshow, but we first have to re-order the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a85017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to change the ordering as the channels should be the last dimensions for pyplot.\n",
    "plt.imshow(img_with_boxes.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116ad45",
   "metadata": {},
   "source": [
    "We can see that the model does a good job at finding the object that are present in the COCO dataset classes, meaning the pre-trained weights are good. This is still a good sanity check that the model weights are meaningful and will hopefully be a good starting point for the creation of my model.\n",
    "\n",
    "### Dataset and Dataloader\n",
    "\n",
    "Now that we have inspected the model and have a better understanding of the architecture, how to use it and the input and output types we are ready to start fine-tuning it. The first step is to get our data loaded in by creating a PyTorch Dataset and then making use of a Dataloader to load the data in in batches.\n",
    "\n",
    "The Dataset class below implements the relevant functions to read our Midog data in as a Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd0b774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset class inheriting from the Dataset\n",
    "class MidogDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class for implementing a dataset for the data for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root, transforms=None):\n",
    "        \"\"\"\n",
    "        root - Path to the directory storing the data\n",
    "        transforms - transform method to be applied to the data\n",
    "        \"\"\"\n",
    "        \n",
    "        # Store the root and transforms as class variables.\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # load all image files, sorting them to ensure that they are aligned.\n",
    "        # Save the images into an instance variable.\n",
    "        self.imgs = list(sorted(os.listdir(Path(root, \"data\"))))\n",
    "        \n",
    "        # Open the training json and load in the data.\n",
    "        with open(os.path.join(root, \"training.json\")) as t:   \n",
    "            training_data = json.load(t)\n",
    "        \n",
    "        # Store the data into an instance variable.\n",
    "        self.data = training_data[\"images\"]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the data for a given index\n",
    "        \n",
    "        idx - The index of the data item to be returned\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the image path and load the image into memory.\n",
    "        img_path = Path(self.root, \"data\", self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Parse the image path to get the image and tild id.\n",
    "        image_id, tile_id = self.imgs[idx].split(\".\")[0].split(\"_\")\n",
    "\n",
    "        # Get the tile info for the given image and tile id.\n",
    "        tile_info = self.get_tile_info(image_id, tile_id)\n",
    "\n",
    "        # Extract the bounding boxes from the tile annotations.\n",
    "        boxes = []\n",
    "        for anno in tile_info[\"annotations\"]:\n",
    "            left, bottom, right, top = anno[\"bounding_box\"].values()\n",
    "            \n",
    "            boxes.append([left, bottom, right, top])\n",
    "        \n",
    "        # Get the number of objects so we can create the labels.\n",
    "        num_objs = len(boxes)\n",
    "        \n",
    "        # There is only one class, mitotic figure, so all labels are 1.\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        # Convert the boxes to a tensor of int32.\n",
    "        # Note: int 16 is enough to hold the information we have but the model expects the targets in int32 format.\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.int32)\n",
    "\n",
    "        # Create the target dictionary to be returned for a data item.\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"tile_id\"] = tile_id\n",
    "    \n",
    "\n",
    "        # If there are any transforms defined we can carry them out here.\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        # The number of data items we have the length of our list of image paths.\n",
    "        # We could have used the individual annotations as a unit of data but this adds additional complexity and \n",
    "        # model training is not the primary objective of this project.\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def get_tile_info(self, image_id, tile_id):\n",
    "        \"\"\"\n",
    "        Return the dictionary that contains information relating to the given image and tile id.\n",
    "        \n",
    "        image_id - The int id of the image we want the data for.\n",
    "        tile_id - The int id of the tile we want the data for.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the dictionary with information on the given image.\n",
    "        image_info = next((image for image in self.data if image[\"image_id\"] == int(image_id)), None)\n",
    "        \n",
    "        # Within this image dictionary, get the dictionary that relates to the given tile id.\n",
    "        tile_info = next((tile for tile in image_info[\"tiles\"] if tile[\"tile_id\"] == int(tile_id)), None)\n",
    "        \n",
    "        return tile_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a40982eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transformation for the dataset so can have the image as a tensor for use with the model.\n",
    "def transforms(img, target):\n",
    "    return T.ToTensor()(img), target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc6fbb0",
   "metadata": {},
   "source": [
    "With our dataset defined, we can now create a dataset object by giving the path to the directory containing the training images and json. Then we can use a DataLoader to iterate through the data and return batches of a specified size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c9f8cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a Dataset object with the given root path to the training data and a defined transformation.\n",
    "midog = MidogDataset(\"D:/DS/DS4/Project/Training_mitotic_figures\", transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader with the dataset with a batch size of 2, no shuffling and use a custom defined collate_fn to batch\n",
    "# the output as desired.\n",
    "data_loader = torch.utils.data.DataLoader(midog, batch_size=2, shuffle=False, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator from the DataLoader and take the next item from the iterator.\n",
    "images, target = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3efe8",
   "metadata": {},
   "source": [
    "Using this batch of images from the dataset we can compute predictions. The classes returned will be meaningless as the COCO dataset that the model was originally trained on does not contain mitotic figure as a class. It will be interesting to see if the model can detect anything of interest in these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b044a2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the images to the model, which is in evaluation mode already.\n",
    "predictions, _ = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de374a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the boxes out of the prediction.\n",
    "boxes = predictions[0][\"boxes\"]\n",
    "\n",
    "# Concatenate the prediction boxes and the ground truth boxes so we can view both.\n",
    "final_boxes = torch.cat([boxes, target[0][\"boxes\"]])\n",
    "\n",
    "# Create a list of labels using the length of the predictions and ground truths.\n",
    "labels = ([\"pred\"] * len(boxes)) + ([\"gt\"] * len(target[0][\"boxes\"]))\n",
    "\n",
    "# Take just the first image.\n",
    "# Note: We can't use squeeze as the first dimension is not 1 this time, but 2 as our batch size is 2.\n",
    "example = images[0,:]\n",
    "\n",
    "# Convert the image to int8.\n",
    "example = T.ConvertImageDtype(torch.uint8)(example) # Convert it to int8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c24844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use draw_bounding_boxes to overlay the bounding boxes and labels on to the image.\n",
    "img_with_boxes = draw_bounding_boxes(example, final_boxes, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to change the ordering as the channels should be the last dimensions for pyplot.\n",
    "plt.imshow(img_with_boxes.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e2af1",
   "metadata": {},
   "source": [
    "Unfortunately, there does not seem to be any meaningful predictions being made by the model with default weights. This is to be expected as the task is in a different domain. Once we have trained the model on our data we will see much more reasonable predictions.\n",
    "\n",
    "### Model Alteration\n",
    "\n",
    "Now we need to make some alterations to our model before we train it. We need to change the output number of classes, so we need to create a new predictor that takes the feature maps and predicts the class and bounding box for an object. This can be done as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5bb1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (mitotic figure) + background\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae96a3a",
   "metadata": {},
   "source": [
    "Now our model is ready to be fine-tuned.\n",
    "\n",
    "### Data sanity check\n",
    "\n",
    "Before training I will do a quick sanity check of the data to ensure it is all as it should be.\n",
    "\n",
    "It turns out that there was a bug in my bounding box generation process that didn't account for tile difference when a bounding box was negative i.e. on the padding to the left or under the original tile. This has been fixed and now my sanity checks are as expected. Helping to preserve my sanity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8208c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use our dataset and defined transformations.\n",
    "dataset = MidogDataset(\"D:/DS/DS4/Project/Training_mitotic_figures\", transforms)\n",
    "\n",
    "# Initiate the DataLoader.\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=False, collate_fn=utils.collate_fn)\n",
    "\n",
    "# For every image and target in the iterator\n",
    "for images, targets in data_loader:\n",
    "    try:\n",
    "        # Check that no values are outside of the boudanries (412 x 412 original and 50 pixel padding all around, so 512 x 512)\n",
    "        assert(max([t[\"boxes\"].numpy().max() for t in targets]) <= 512)\n",
    "        assert(min([t[\"boxes\"].numpy().min() for t in targets]) >= 0)\n",
    "\n",
    "        # Stack all of the boxes vertically.\n",
    "        boxes = np.vstack([t[\"boxes\"].numpy() for t in targets])\n",
    "\n",
    "        # Check that the x1 < x2 for all of these boxes.\n",
    "        assert(np.all(boxes[:,0] < boxes[:,2]) == True)\n",
    "\n",
    "        # Check that y1 < y2 for all of these boxes.\n",
    "        assert(np.all(boxes[:,1] < boxes[:,3]) == True)\n",
    "    \n",
    "    # If we encounter an AssertionError, print the dictionary of annotations that caused it.\n",
    "    except AssertionError:\n",
    "        print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb526b",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "The process of training our model consists of multiple steps.\n",
    "\n",
    "Firstly we create our dataset and split it into a train and test set. We can make use of torch.manual_seed to ensure that the random premutation we get for the indices stays the same on every run. There are 7549 training images, some of these may have more than 1 example on them. Let us take ~10% of the data for testing, which is 750 tiles for testing (375 batches of 2 images).\n",
    "\n",
    "Now that we have our data, we need to consider the loss. This is returned in the form of a classification and regression loss for the RPN and the R-CNN. These can be summed to get the total loss which can be used to compute the optimization steps.\n",
    "\n",
    "We will make use of the Adam optimizer, which makes use of both Momentum and root mean square propagation to converge faster.\n",
    "\n",
    "This covers our main training loop that updates our weights. After every epoch we would like to run the model on our test set and look at some metrics. We will make use of TorchMetrics to compute the mAP (MeanAveragePrecision) for the test set. In addition, looking at the test loss will allow us to know when we should stop training. In this way, we can avoid overfitting the training data.\n",
    "\n",
    "TorchMetrics is easy to use and involves passing the ground truths and predictions to the object created from the MeanAveragePrecision class. Returning the test loss is a different story. The classes for the components of the model in Torchvision don't return the loss when in evaluation. To remedy this, I altered the logic in these files to ensure that if the ground truths are provided the loss is returned. This preseves the previous functionality while allowing me to return the loss.\n",
    "\n",
    "The below code is the original final output from the GeneralizedRCNN class:\n",
    "\n",
    "``` \n",
    "def eager_outputs(self, losses, detections):\n",
    "        # type: (Dict[str, Tensor], List[Dict[str, Tensor]]) -> Union[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
    "        if self.training:\n",
    "            return losses\n",
    "\n",
    "        return detections\n",
    "```\n",
    "\n",
    "This is what I changed it to:\n",
    "``` \n",
    "def eager_outputs(self, losses, detections):\n",
    "        # type: (Dict[str, Tensor], List[Dict[str, Tensor]]) -> Union[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
    "        if self.training:\n",
    "            return losses\n",
    "\n",
    "        elif losses:\n",
    "            return losses, detections\n",
    "\n",
    "        return detections\n",
    "``` \n",
    "\n",
    "Changes also had to be made to the logic in the components of the GeneralizedRCNN, namely the classes in roi_heads.py and rpn.py. These altered files will be included in the repositiory so the test loss can be computed during the following training.\n",
    "\n",
    "With all of this, we are ready to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c81636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorBoard SummaryWriter so we can write metrics to the TensorBoard logs for monitoring.\n",
    "# Note: This was not useful as sessions don't seem to work well when the SummaryWriter is initialized again, when training at\n",
    "# another point in time.\n",
    "writer = SummaryWriter('D:/DS/DS4/Project/tensorboard_logs/midog_training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450e1ce8",
   "metadata": {},
   "source": [
    "Now we can load our dataset and pass it to our train method. This method and other methods that are called within train can be found under Training in the Utils folder of the projecy repo.\n",
    "\n",
    "The model can be trained from scratch, or a checkpoint file from torch.save can be passed that contains the model state_dict, the optimizer state_dict and the current epoch.\n",
    "\n",
    "Using all of the data takes roughly 1 hour and 30 minutes to train one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de46bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MidogDataset(\"D:/DS/DS4/Project/Training_mitotic_figures\", transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9687cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training from scratch, no checkpoint is given.\n",
    "train(model, dataset, 10, \"D:/DS/DS4/Project/model_saves\", writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85580315",
   "metadata": {},
   "source": [
    "If we would like to continue training from a checkpoint we can use the below code and read in the checkpoint and pass it to the train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef173ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training from a checkpoint, which is passed as the last parameter to train.\n",
    "checkpoint = torch.load(\"D:/DS/DS4/Project/model_saves/2023_03_02_16_35_46_8.pth\", map_location=lambda storage, loc: storage)\n",
    "train(model, dataset, 10, \"D:/DS/DS4/Project/model_saves\", writer, checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c72c687",
   "metadata": {},
   "source": [
    "### Training overview\n",
    "\n",
    "Once we have trained our model, we can look at the training and test loss to determine the best model for us to use. To do this, we can read in the metrics file and extract the train and test loss. This will only involve reading the text in and parsing it to extract these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48e43d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the metrics file\n",
    "with open(\"D:/DS/DS4/Project/model_saves/final_metrics.txt\") as metrics:\n",
    "    data = metrics.read()\n",
    "\n",
    "# Extract the lines relating to loss\n",
    "loss_entries = data.split(\"\\n\")[0::3]\n",
    "\n",
    "# Split the line on \" loss: \"\n",
    "split_losses = [epoch.split(\" loss: \") for epoch in loss_entries]\n",
    "\n",
    "# Variables for storing the losses\n",
    "training_loss, test_loss = [], []\n",
    "\n",
    "for loss_list in split_losses:\n",
    "    # Append the training loss from the second element after splitting the whitespace.\n",
    "    training_loss.append(float(loss_list[1].split(\" \")[0]))\n",
    "    \n",
    "    # Take the last element as the test loss.\n",
    "    test_loss.append(float(loss_list[2]))\n",
    "\n",
    "# This is a list of strings that represent the MeanAveragePrecision dictionary.\n",
    "# So we will need to convert this back to a dictionary\n",
    "metrics = [metric for metric in data.split(\"\\n\")[1::3]]# [literal_eval(metric) for metric in data.split(\"\\n\")[1::3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5826e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_loss)\n",
    "plt.plot(test_loss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3578bc",
   "metadata": {},
   "source": [
    "We can see that there is no drastic changes in the test loss, so we could continue training. Though, there will be marginal improvement at the cost of lots of computational power. So I believe this training is sufficient for our usecase and we can take the model at epoch 11, which has the lowest test loss and best COCO mAP metrics.\n",
    "\n",
    "In order to extract the mAP metrics, we can either view the raw string, or use eval to take the string as Python code. There is one small issue with this approach considering our format. The values are stored in tensors, denoted by the tensor(value). This is not a defined funtion in Python, so we can use an adhoc solution which is to define such a function that returns the value passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the metrics string.\n",
    "metrics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5443025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy tensor function to return the value passed when using eval on the metric string.\n",
    "def tensor(value):\n",
    "    return value\n",
    "\n",
    "# Let us take the metrics at epoch 11.\n",
    "metric_dict = eval(metrics[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1012c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecc6bb7",
   "metadata": {},
   "source": [
    "We can see that the metrics are decent and the model has a reasonable mAP considering there is only one class. There may be changes that could be made to the learning rate during training, but this is not significant for my usecase.\n",
    "\n",
    "### Review trained model predictions\n",
    "\n",
    "We can now load the model and see how well the predictions look visually for images from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6c1cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the best model by specifying the path to the save.\n",
    "checkpoint = torch.load(\"D:/DS/DS4/Project/model_saves/2023_03_04_17_22_33_11.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29f23696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): FastRCNNConvFCHead(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Flatten(start_dim=1, end_dim=-1)\n",
       "      (5): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the model that we have already initialized and load the state_dict into it.\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "# Set it to evaluation mode.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76b23b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our dataset and defined transformations.\n",
    "dataset = MidogDataset(\"D:/DS/DS4/Project/Training_mitotic_figures\", transforms)\n",
    "\n",
    "# Split the dataset into the train and test set, using the same random seed to ensure the same split.\n",
    "torch.manual_seed(42)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset_test = torch.utils.data.Subset(dataset, indices[-750:])\n",
    "\n",
    "# Create the DataLoader for the test dataset.\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "# Create an iterator from the DataLoader.\n",
    "iter_data = iter(data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d872a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the next batch from the DataLoader\n",
    "images, targets = next(iter_data)\n",
    "\n",
    "# Use the GPU if it is available.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Move both the model and the data to the device, ideally the GPU.\n",
    "model.to(device)\n",
    "images = images.to(device)\n",
    "targets = [{k: v.to(device) for k, v in t.items() if k in [\"boxes\", \"labels\"]} for t in targets]\n",
    "\n",
    "# Don't calculate gradients as we aren't updating the weights and this will save memory.\n",
    "with torch.no_grad():\n",
    "    predictions, class_logits = model(images)\n",
    "\n",
    "print(class_logits)\n",
    "\n",
    " # Pull the boxes out of the prediction.\n",
    "boxes = predictions[0][\"boxes\"]\n",
    "\n",
    "# Concatenate the prediction and ground truth bounding boxes.\n",
    "final_boxes = torch.cat([boxes, targets[0][\"boxes\"]])\n",
    "\n",
    "# Create a list of labels using the length of the list of predictions and list of ground truths.\n",
    "labels = ([\"pred\"] * len(boxes)) + ([\"gt\"] * len(targets[0][\"boxes\"]))\n",
    "\n",
    "# Reduce the dimension to just channel x height x width as we have only one image in the batch and want to display it.\n",
    "example = torch.squeeze(images)\n",
    "\n",
    "# Convert the image to int8\n",
    "example = T.ConvertImageDtype(torch.uint8)(example)\n",
    "\n",
    "\n",
    "img_with_boxes = draw_bounding_boxes(example, final_boxes, labels)\n",
    "\n",
    "# We need to change the ordering as the channels should be the last dimensions for pyplot.\n",
    "plt.imshow(img_with_boxes.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a0a692",
   "metadata": {},
   "source": [
    "We can see from running the last cell several times that the ground truth is almost always detected by the model and typically with a high confidence in the prediction. This shows to me that the model has learnt some of the intricacies of what makes mitotic figures stand out from other cells or artefacts within a tile from a whole slide image.\n",
    "\n",
    "### Extracting snippets of annotations for concepts\n",
    "\n",
    "For generating concepts from the mitotic figures I will need to extract the annotations in the bounding boxes so visual concepts can be discovered more easily. If I don't carry this out, concepts relating to the visual features of the entire tile will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14f6cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = MidogDataset(\"D:/DS/DS4/Project/Training_mitotic_figures\")\n",
    "\n",
    "# Set the output path and create a directory\n",
    "output_path = Path(\"D:/DS/DS4/Project/Mitotic_figures\")\n",
    "output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Iterate over the images and targets\n",
    "for image, target in tqdm(dataset, total=len(dataset)):\n",
    "    \n",
    "    # For every bounding box in the annotation, create a snippet.\n",
    "    for i, box in enumerate(target[\"boxes\"]):\n",
    "        \n",
    "        # Extract the box coordinates \n",
    "        left, bottom, right, top = box\n",
    "        left, bottom, right, top = left.item(), bottom.item(), right.item(), top.item()\n",
    "        snippet = image.crop(box=(left, bottom, right, top))\n",
    "        \n",
    "        # Get the identifiers\n",
    "        image_id = target[\"image_id\"]\n",
    "        tile_id = target[\"tile_id\"]\n",
    "        \n",
    "        # Save the snippet using a unique name consisting of the image_id, tile_id and box number.\n",
    "        snippet.save(output_path / f\"{image_id}_{tile_id}_{i}.png\", format=\"PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc465a8",
   "metadata": {},
   "source": [
    "### Testing extraction of the activations and gradients for generating and testing CAVs\n",
    "\n",
    "The process of creating concepts vectors involves the use of activations from the bottleneck layers in the model. These are used to create a linear classifier whose decision boundary is used to find the concept activation vector. The concept activation vector is the vector orthogonal to the decision boundary.\n",
    "\n",
    "Once the CAV is found, the influence of a CAV on a prediction can be found by taking the gradients of a certain class with respect to the activations in the bottleneck layer.\n",
    "\n",
    "So we must extract the activations and the gradients so we can make use of the TCAV method.\n",
    "\n",
    "#### Extracting the activations\n",
    "\n",
    "We can make use of a feature extractor to efficiently extract the activations that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the next batch from the DataLoader\n",
    "images, _ = next(iter_data)\n",
    "images = images.to(device)\n",
    "\n",
    "# Use the GPU if it is available.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3735e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35174397",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bottlenecks = ['body.layer1.2.conv1', 'body.layer2.3.conv1', 'body.layer3.5.conv1', 'body.layer4.2.conv1']\n",
    "fe = create_feature_extractor(model.backbone, bottlenecks)\n",
    "fe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = fe(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a123a1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "activations[\"body.layer2.3.conv1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cb1076",
   "metadata": {},
   "source": [
    "We can see that we have extracted the activations in the given layers for the image we passed through the feature extractor.\n",
    "\n",
    "#### Extracting the gradients\n",
    "\n",
    "Now we can extract the gradients, this will involve doing a forward pass of the model, extracting the class logits and performing a backward() call. The gradients can then be extracted from the relevant layers as needed, assuming they have require_grad=True.\n",
    "\n",
    "Extracting the class logits invovled manipulating the default code for the model in the torchvision library. The amended files are available in the Utils/torchvision files folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb58cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the next batch from the DataLoader\n",
    "images, _ = next(iter_data)\n",
    "\n",
    "# Use the GPU if it is available.\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Move both the model and the data to the device, ideally the GPU.\n",
    "model.to(device)\n",
    "images = images.to(device)\n",
    "\n",
    "# Calculate the gradients as we need to extract them afterwards\n",
    "predictions, class_logits = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bc90389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4027,  1.3828]], device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the class logits to see how many sets/ predictions were returned.\n",
    "class_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4651e893",
   "metadata": {},
   "source": [
    "We need to check that we have only one set of class logits, corresponding to one bounding box. If this is not the case we have to process the class logit sets one at a time to get the gradients for each prediction.\n",
    "\n",
    "Now we call .backward() in the class logit corresponding to the class we want to test the sensitivity of in relation to concepts. In our case we have background and mitotic figure as classes, so we will look at class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0ff49e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call backwards on the class logit for class k, where k = 1\n",
    "class_logits[:,1].backward(retain_graph=True)\n",
    "\n",
    "# If we have multiple sets of class logits.\n",
    "# class_logits[:, 1][0].backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85e7eb14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the unique values in the gradient of the box preditor cls score.\n",
    "np.unique(model.roi_heads.box_predictor.cls_score.weight.grad.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5c3910",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the unique values in the gradient of the bottleneck in layer2, block 3.\n",
    "np.unique(model.backbone.body.layer2[3].conv1.weight.grad.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7a1fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero all of the gradients in the model so we can compute a gradient for a different class logit\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faabdaf5",
   "metadata": {},
   "source": [
    "So we can compute and access both the activations and the gradients of our model while passing input forward and class logits backwards. These can be used to generate concept activation vectors and test the sentivity of class predictions to these concepts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
