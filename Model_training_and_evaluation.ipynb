{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2248f98",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "This notebook contains all of the code used to train the model used in my project. This model makes use of the PyTorch library and the pre-trained Faster R-CNN model provided in the TorchVision library. The structure of the model can be changed to accomadate the different output classes and then finetuned on my training data. The documentation for the Faster R-CNN implementation can be found [here](https://pytorch.org/vision/stable/models/faster_rcnn.html).\n",
    "\n",
    "The primary components of this model are a region proposal network (RPN), a Fast-RCNN classifier and a backbone. The pytorch stable 1.13.1 and cuda 11.7 versions were used in this environment.\n",
    "\n",
    "- test model\n",
    "- dataset and loader\n",
    "- Alter structure\n",
    "- Train\n",
    "- Evaluate\n",
    "\n",
    "### Imports\n",
    "\n",
    "The following imports are needed to run the code in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "067def1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as f\n",
    "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8f04c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('D:/DS/DS4/Project/tensorboard_logs/midog')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f004543b",
   "metadata": {},
   "source": [
    "Ensure that we have our cuda set up correctly and have access to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d209dc11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1050 Ti'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb371f",
   "metadata": {},
   "source": [
    "### Model inspection\n",
    "\n",
    "Firstly, I will load the default model, inspect the architecture and ensure that it works as expected. This can be done by using the model to make a prediction on an image from the COCO dataset (which it was trained on). In this way I can better understand how the model should be used and ensure it has the pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights into the model and save as an object. Set the model to evaluation mode so we can inference.\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de58d363",
   "metadata": {},
   "source": [
    "We have loaded the weights into the model object and set the model to evaluation mode. Now we can use print on the model to output all of the information regarding the layers and different connections. We can see several different componenets.\n",
    "\n",
    "+ Transform - the transformations the be applied to an image before it can be passed through the model.\n",
    "+ backbone (with FPN) - This part of the model is responsible for feature extraction and consists of sequential bottlenecks to create compressed feature representations. The feature pyramid network (FPN) allows for context at different levels by outputing feature maps at multiple levels.\n",
    "+ Region proposal network (RPN) = Network for proposing regions with possible object of interest that are used during the final stage of detection.\n",
    "+ Fast R-CNN predictor - Takes the regions and the features maps and returns the detections.\n",
    "\n",
    "This is the basic overview of our model that can be seen in the below output. This can be finetuned to our specific task with some minor alterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d28db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13f25c",
   "metadata": {},
   "source": [
    "Now we can take a sample image from the COCO dataset, which the default weights for the Faster R-CNN model were trained on. Using this we can see how the model works and how well it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1d043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a COCO image\n",
    "img = Image.open(\"D:/DS/DS4/Project/COCO/tennis racket/COCO_test2014_000000000057.jpg\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb0961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the image\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49229597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to a tensor and add an empty dimension so we have [number images x channels x height x width]\n",
    "tensor_img = T.ToTensor()(img)\n",
    "tensor_img = tensor_img[None, :]\n",
    "print(tensor_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass our example image through the model to get the predictions.\n",
    "_, predictions = model(tensor_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae565e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = predictions[0][\"boxes\"] # Pull the boxes out of the prediction\n",
    "labels = [\"person\", \"sports ball\", \"tennis racket\", \"clock\", \"sports ball\"] # These labels were manually entered from the COCO dataset class list\n",
    "example = tensor_img[0,:] # Take just the first image\n",
    "example = T.ConvertImageDtype(torch.uint8)(example) # Convert it to int8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2cf3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_with_boxes = draw_bounding_boxes(example, boxes, labels) # Draw the bounding boxes onto the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a85017",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_with_boxes.permute(1, 2, 0)) # We need to change the ordering as the channels should be the last dimensions for pyplot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b116ad45",
   "metadata": {},
   "source": [
    "We can see that the model does a good job at finding the object that are present in the COCO dataset classes, and it is likely the weights were trained on this image. This is still a good sanity check that the model weights are meaningful and will hopefully be a good starting point for the creation of my model.\n",
    "\n",
    "### Dataset and Dataloader\n",
    "\n",
    "Now that we have inspected the model and have a better understanding of the architecture, how to use it and the input and output types we are ready to start fine-tuning it. The first step is to get our data loaded in by creating a PyTorch Dataset and then making use of a Dataloader to load the data in in batches.\n",
    "\n",
    "The Dataset class below implements the relevant functions to read our Midog data in as a Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a40982eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(img, target):\n",
    "    return T.ToTensor()(img), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd0b774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidogDataset(Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(Path(root, \"data\"))))\n",
    "        \n",
    "        with open(os.path.join(root, \"training.json\")) as t:   \n",
    "            trining_data = json.load(t)\n",
    "        \n",
    "        self.data = trining_data[\"images\"]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = Path(self.root, \"data\", self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        image_id, tile_id = self.imgs[idx].split(\".\")[0].split(\"_\")\n",
    "\n",
    "        tile_info = self.get_tile_info(image_id, tile_id)\n",
    "\n",
    "        # get bounding box coordinates for this tile\n",
    "        boxes = []\n",
    "        for anno in tile_info[\"annotations\"]:\n",
    "            left, bottom, right, top = anno[\"bounding_box\"].values()\n",
    "            \n",
    "            boxes.append([left, bottom, right, top])\n",
    "        \n",
    "        num_objs = len(boxes)\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.int32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "    \n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"tile_id\"] = tile_id\n",
    "    \n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def get_tile_info(self, image_id, tile_id):\n",
    "        \n",
    "        image_info = next((image for image in self.data if image[\"image_id\"] == int(image_id)), None)\n",
    "        tile_info = next((tile for tile in image_info[\"tiles\"] if tile[\"tile_id\"] == int(tile_id)), None)\n",
    "        \n",
    "        return tile_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c9f8cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "midog = MidogDataset(\"D:/DS/DS4/Project/Training_mitotic_figures\", transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(midog, batch_size=2, shuffle=False, collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, target = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3efe8",
   "metadata": {},
   "source": [
    "Using this batch of images we can compute predictions and see if the model can detect any objects. The classes will be meaningless as they are from another task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b044a2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de374a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = predictions[0][\"boxes\"] # Pull the boxes out of the prediction\n",
    "final_boxes = torch.cat([boxes, target[0][\"boxes\"]])\n",
    "labels = [\"pred\", \"pred\", \"pred\", \"pred\", \"pred\", \"pred\",\"gt\"]\n",
    "example = images[0,:] # Take just the first image\n",
    "example = T.ConvertImageDtype(torch.uint8)(example) # Convert it to int8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c24844",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_with_boxes = draw_bounding_boxes(example, final_boxes, labels) # Draw the bounding boxes onto the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_with_boxes.permute(1, 2, 0)) # We need to change the ordering as the channels should be the last dimensions for pyplot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e2af1",
   "metadata": {},
   "source": [
    "Unfortunately, there does not seem to be any meaningful predictions being made by the model with default weights. This is to be expected as the task is in a different domain. Once we have trained the model on our data we will see much more reasonable predictions.\n",
    "\n",
    "### Model Alteration\n",
    "\n",
    "No we need to make some alterations to our model before we train it. We need to change the output number of classes, so we need to create a new predictor that takes the feature maps and predicts the class and bounding box for an object. This can be done as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5bb1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=\"DEFAULT\")\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (mitotic figure) + background\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae96a3a",
   "metadata": {},
   "source": [
    "Now our model is ready to be fine-tuned.\n",
    "\n",
    "### Data sanity check\n",
    "\n",
    "Before training I will do a quick sanity check of the data to ensure it is all as it should be. It turns out that there was a bug in my bounding box generation process that didn't account for tile difference when a bounding box was negative i.e. on the padding to the left or under the original tile. This has been fixed and now my sanity checks are as expected. Preserving my sanity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a8208c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = MidogDataset(\"D:/DS/DS4/Project/Training_mitotic_figures\", transforms)\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=False, collate_fn=utils.collate_fn)\n",
    "\n",
    "for images, targets in data_loader:\n",
    "    try:\n",
    "        # Check that no values are outside of the boudanries\n",
    "        assert(max([t[\"boxes\"].numpy().max() for t in targets]) < 512)\n",
    "        assert(min([t[\"boxes\"].numpy().min() for t in targets]) >= 0)\n",
    "\n",
    "        boxes = np.vstack([t[\"boxes\"].numpy() for t in targets])\n",
    "\n",
    "        # Check that the x1 < x2\n",
    "        assert(np.all(boxes[:,0] < boxes[:,2]) == True)\n",
    "\n",
    "        # Check that y1 < y2\n",
    "        assert(np.all(boxes[:,1] < boxes[:,3]) == True)\n",
    "    \n",
    "    except AssertionError:\n",
    "        print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb526b",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "The process of training our model consists of multiple steps.\n",
    "\n",
    "Firstly we create our dataset and split it into a train and test set. We can make use of the torch random seed to ensure that the random premutation we get for the indices stays the same everytime this is run. There are 7549 training images, some of these may have more than 1 example on them. Let us take ~10% of the data for testing, which is 750 tiles for testing (375 batches of 2 images).\n",
    "\n",
    "Now that we have our data, we need to consider the loss. This is returned in the form of a classification and regression loss for the RPN and the R-CNN. These can be summed to get the total loss which can be used to compute the optimization steps.\n",
    "\n",
    "We will make use of the Adam optimizer, which makes use of both Momentum and root mean square propagation to converge faster.\n",
    "\n",
    "This covers our main training loop that updates our weights. After every epoch we would like to run the model on our test set and look at some metrics. We will make use of a sample test set to determine the metrics we wish to use. These will come from the torchmetrics library and will be commonly used metrics for detection tasks. In addition, it would be nice to look at the test loss. \n",
    "\n",
    "To find this we need to alter the code in the torchvision library as this pre-trained model does not return the loss in evaluation mode. The files that have been altered are available in the project repo. The changed invovled going through the Faster R-CNN component classes and ensuring that they returned the loss in evaluation mode.\n",
    "\n",
    "With this, we are ready to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a18bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
    "    model.train()\n",
    "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
    "    header = f\"Epoch: [{epoch}]\"\n",
    "\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "\n",
    "    total_loss = 0\n",
    "    for iteration, data in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "        images, targets = data\n",
    "        images = images.to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items() if k in [\"boxes\", \"labels\"]} for t in targets]\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "        total_loss += loss_value\n",
    "        \n",
    "        del images, targets, loss_dict\n",
    "        \n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if scaler is not None:\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "    \n",
    "    writer.add_scalar('training loss', total_loss / len(data_loader), epoch)\n",
    "\n",
    "    return metric_logger, total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7294013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, data_loader_test, epoch, device):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    metric = MeanAveragePrecision()\n",
    "    metric.to(device)\n",
    "    \n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader_test):\n",
    "            images = images.to(device)\n",
    "\n",
    "            targets = [{k: v.to(device) for k, v in t.items() if k in [\"boxes\", \"labels\"]} for t in targets]\n",
    "\n",
    "            losses, predictions = model(images, targets)\n",
    "            losses_summed = sum(loss for loss in losses.values()).item()\n",
    "            total_loss += losses_summed\n",
    "\n",
    "            metric.update(predictions, targets)\n",
    "\n",
    "            del predictions, images, targets, losses\n",
    "    \n",
    "    test_loss = total_loss / len(data_loader_test)\n",
    "    writer.add_scalar('test loss', total_loss / test_loss, epoch)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    test_metrics = metric.compute()\n",
    "    \n",
    "    return test_loss, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4859068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def train(model, num_epochs, checkpoint=None):\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    # use our dataset and defined transformations\n",
    "    dataset = MidogDataset(\"D:/DS/DS4/Project/Training_mitotic_figures\", transforms)\n",
    "\n",
    "    # split the dataset in train and test set\n",
    "    torch.manual_seed(42)\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    dataset_train = torch.utils.data.Subset(dataset, indices[:-750])\n",
    "    dataset_test = torch.utils.data.Subset(dataset, indices[-750:])\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    # define training and validation data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train, batch_size=2, shuffle=False, collate_fn=utils.collate_fn)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=2, shuffle=False,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    if checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(params, lr=0.005, weight_decay=0.0005)\n",
    "    \n",
    "    if checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "    epoch = 0\n",
    "    \n",
    "    if checkpoint:\n",
    "        epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    for epoch in range(epoch, epoch + num_epochs):\n",
    "        # train for one epoch, printing every 100 iterations\n",
    "        _, training_loss = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\n",
    "        # update the learning rate\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        test_loss, test_metrics = evaluate(model, data_loader, data_loader_test, epoch, device=device)\n",
    "        \n",
    "        with open(\"D:/DS/DS4/Project/model_saves/metrics.txt\", \"a\") as m:\n",
    "            m.write(f\"Epoch {epoch}: \" + \"Training loss: {training_loss} Test loss: {test_loss}\\n\" + str(test_metrics) + \"\\n\\n\")\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "        path_to_save = Path(\"D:/DS/DS4/Project/model_saves\") / f\"{timestamp}_{epoch}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9687cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [   0/3400]  eta: 2:33:30  lr: 0.000010  loss: 0.8816 (0.8816)  loss_classifier: 0.6656 (0.6656)  loss_box_reg: 0.0023 (0.0023)  loss_objectness: 0.1966 (0.1966)  loss_rpn_box_reg: 0.0171 (0.0171)  time: 2.7090  data: 0.0468  max mem: 2771\n"
     ]
    }
   ],
   "source": [
    "# Initial training\n",
    "train(model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac08dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training from checkpoint\n",
    "checkpoint = torch.load(\"D:/DS/DS4/Project/model_saves/2023_02_27_16_13_14_0.pth\")\n",
    "train(model, 10, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b92024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbab6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"D:/DS/DS4/Project/model_saves/2023_02_27_17_44_05_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb92ae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = MidogDataset(\"D:/DS/DS4/Project/Training_mitotic_figures\", transforms)\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(42)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset_test = torch.utils.data.Subset(dataset, indices[-750:])\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece7bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_data = iter(data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa77bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter_data)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "images = images.to(device)\n",
    "targets = [{k: v.to(device) for k, v in t.items() if k in [\"boxes\", \"labels\"]} for t in targets]\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(images)\n",
    "    \n",
    "boxes = predictions[0][\"boxes\"] # Pull the boxes out of the prediction\n",
    "final_boxes = torch.cat([boxes, targets[0][\"boxes\"]])\n",
    "labels = ([\"pred\"] * len(boxes)) + ([\"gt\"] * len(targets[0][\"boxes\"]))\n",
    "example = images[0,:] # Take just the first image\n",
    "example = T.ConvertImageDtype(torch.uint8)(example) # Convert it to int8 \n",
    "\n",
    "img_with_boxes = draw_bounding_boxes(example, final_boxes, labels) # Draw the bounding boxes onto the image\n",
    "\n",
    "plt.imshow(img_with_boxes.permute(1, 2, 0)) # We need to change the ordering as the channels should be the last dimensions for pyplot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
